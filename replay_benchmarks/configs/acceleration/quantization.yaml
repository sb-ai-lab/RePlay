acceleration:
  model:
    acceleration_config:
      dtype: fp32 #fp32, fp16, bf16 can be used for loading model weights
      transformer_block: 
        pff_block:
          act_fn: gelu # gelu, gelu_pytorch_tanh, silu, relu
      head: swichback_head
