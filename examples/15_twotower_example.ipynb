{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of TwoTower training/inference\n",
    "\n",
    "This example uses `ParquetModule` for data loading and processing. \n",
    "\n",
    "**It requires setting environment variables OMP_NUM_THREADS and ARROW_IO_THREADS to match the number of available CPUs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_NUM_THREADS=8\n",
      "env: ARROW_IO_THREADS=8\n"
     ]
    }
   ],
   "source": [
    "%env OMP_NUM_THREADS=8\n",
    "%env ARROW_IO_THREADS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "import pandas as pd\n",
    "\n",
    "L.seed_everything(42);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data\n",
    "In this example, we will be using the MovieLens dataset, namely the 1m subset. It's demonstrated a simple case, so only item ids will be used as model input.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "Current implementation of TwoTower is able to handle item and interactions features. It does not take into account user features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_csv(\"./data/ml1m_ratings.dat\", sep=\"\\t\", names=[\"user_id\", \"item_id\",\"rating\",\"timestamp\"])\n",
    "interactions = interactions.drop(columns=[\"rating\"])\n",
    "\n",
    "item_features = pd.read_csv(\"./data/ml1m_items.dat\", sep=\"\\t\", names=[\"item_id\", \"title\", \"genres\"])\n",
    "item_features = item_features.drop(columns=[\"title\", \"genres\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000138</th>\n",
       "      <td>6040</td>\n",
       "      <td>858</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000153</th>\n",
       "      <td>6040</td>\n",
       "      <td>2384</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999873</th>\n",
       "      <td>6040</td>\n",
       "      <td>593</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000192</th>\n",
       "      <td>6040</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000007</th>\n",
       "      <td>6040</td>\n",
       "      <td>1961</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825793</th>\n",
       "      <td>4958</td>\n",
       "      <td>2399</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825438</th>\n",
       "      <td>4958</td>\n",
       "      <td>1407</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825731</th>\n",
       "      <td>4958</td>\n",
       "      <td>2634</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825724</th>\n",
       "      <td>4958</td>\n",
       "      <td>3264</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825603</th>\n",
       "      <td>4958</td>\n",
       "      <td>1924</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000209 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id  timestamp\n",
       "1000138     6040      858          0\n",
       "1000153     6040     2384          1\n",
       "999873      6040      593          2\n",
       "1000192     6040     2019          3\n",
       "1000007     6040     1961          4\n",
       "...          ...      ...        ...\n",
       "825793      4958     2399        446\n",
       "825438      4958     1407        447\n",
       "825731      4958     2634        448\n",
       "825724      4958     3264        449\n",
       "825603      4958     1924        450\n",
       "\n",
       "[1000209 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions[\"timestamp\"] = interactions[\"timestamp\"].astype(\"int64\")\n",
    "interactions = interactions.sort_values(by=\"timestamp\")\n",
    "interactions[\"timestamp\"] = interactions.groupby(\"user_id\").cumcount()\n",
    "interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode catagorical data.\n",
    "To ensure all categorical data is fit for training, it needs to be encoded using the `LabelEncoder` class. Create an instance of the encoder, providing a `LabelEncodingRule` for each categorcial column in the dataset that will be used in model. Note that ids of users and ids of items are always used.\n",
    "\n",
    "Let's train the encoder for the `item_id` column using the item features, because some items may be missing from interactions. The `user_id` column will be trained using interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6039</td>\n",
       "      <td>847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6039</td>\n",
       "      <td>2315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6039</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6039</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6039</td>\n",
       "      <td>1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000204</th>\n",
       "      <td>446</td>\n",
       "      <td>4957</td>\n",
       "      <td>2330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000205</th>\n",
       "      <td>447</td>\n",
       "      <td>4957</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000206</th>\n",
       "      <td>448</td>\n",
       "      <td>4957</td>\n",
       "      <td>2565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000207</th>\n",
       "      <td>449</td>\n",
       "      <td>4957</td>\n",
       "      <td>3195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000208</th>\n",
       "      <td>450</td>\n",
       "      <td>4957</td>\n",
       "      <td>1855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000209 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         timestamp  user_id  item_id\n",
       "0                0     6039      847\n",
       "1                1     6039     2315\n",
       "2                2     6039      589\n",
       "3                3     6039     1950\n",
       "4                4     6039     1892\n",
       "...            ...      ...      ...\n",
       "1000204        446     4957     2330\n",
       "1000205        447     4957     1384\n",
       "1000206        448     4957     2565\n",
       "1000207        449     4957     3195\n",
       "1000208        450     4957     1855\n",
       "\n",
       "[1000209 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from replay.preprocessing import LabelEncoder, LabelEncodingRule\n",
    "\n",
    "encoder = LabelEncoder(\n",
    "    [\n",
    "        LabelEncodingRule(\"user_id\"),\n",
    "        LabelEncodingRule(\"item_id\"),\n",
    "    ]\n",
    ")\n",
    "encoder.rules[0].fit(interactions)\n",
    "encoder.rules[1].fit(item_features)\n",
    "encoded_interactions = encoder.transform(interactions)\n",
    "encoded_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>3878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>3879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>3880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3881</th>\n",
       "      <td>3881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3882</th>\n",
       "      <td>3882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3883 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id\n",
       "0           0\n",
       "1           1\n",
       "2           2\n",
       "3           3\n",
       "4           4\n",
       "...       ...\n",
       "3878     3878\n",
       "3879     3879\n",
       "3880     3880\n",
       "3881     3881\n",
       "3882     3882\n",
       "\n",
       "[3883 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features_encoded = encoder.rules[1].transform(item_features)\n",
    "item_features_encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split interactions into the train, validation and test datasets using LastNSplitter\n",
    "We use widespread splitting strategy Last-One-Out. We filter out cold items and users for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.splitters import LastNSplitter\n",
    "\n",
    "splitter = LastNSplitter(\n",
    "    N=1,\n",
    "    divide_column=\"user_id\",\n",
    "    query_column=\"user_id\",\n",
    "    strategy=\"interactions\",\n",
    "    drop_cold_users=True,\n",
    "    drop_cold_items=True\n",
    ")\n",
    "\n",
    "test_events, test_gt = splitter.split(encoded_interactions)\n",
    "validation_events, validation_gt = splitter.split(test_events)\n",
    "train_events = validation_events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing (\"baking\")\n",
    "SasRec expects each user in the batch to provide their events in form of a sequence. For this reason, the event splits must be properly processed using the `groupby_sequences` function provided by RePlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[3117, 1250, 1009, 1672, 2271, 1768, 3339, 118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[1180, 1192, 1199, 2648, 1273, 2874, 1207, 315...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[589, 2789, 1899, 3465, 1407, 1892, 1246, 1358...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[1192, 1081, 3458, 476, 3399, 257, 1180, 1178,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[2648, 907, 896, 352, 1230, 2119, 2789, 1111, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6035</th>\n",
       "      <td>6035</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[1672, 1814, 3369, 2307, 2359, 2503, 2423, 278...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6036</th>\n",
       "      <td>6036</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[693, 1813, 3439, 1959, 1247, 558, 847, 3079, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6037</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[3327, 908, 1192, 2077, 1366, 352, 1063, 1132,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>6038</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[109, 279, 1998, 1211, 918, 3064, 935, 3019, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6039</th>\n",
       "      <td>6039</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[847, 2315, 589, 1950, 1892, 3042, 211, 3436, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6040 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                                          timestamp  \\\n",
       "0           0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "1           1  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "2           2  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "3           3  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "4           4  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "...       ...                                                ...   \n",
       "6035     6035  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "6036     6036  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "6037     6037  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "6038     6038  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "6039     6039  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "\n",
       "                                                item_id  \n",
       "0     [3117, 1250, 1009, 1672, 2271, 1768, 3339, 118...  \n",
       "1     [1180, 1192, 1199, 2648, 1273, 2874, 1207, 315...  \n",
       "2     [589, 2789, 1899, 3465, 1407, 1892, 1246, 1358...  \n",
       "3     [1192, 1081, 3458, 476, 3399, 257, 1180, 1178,...  \n",
       "4     [2648, 907, 896, 352, 1230, 2119, 2789, 1111, ...  \n",
       "...                                                 ...  \n",
       "6035  [1672, 1814, 3369, 2307, 2359, 2503, 2423, 278...  \n",
       "6036  [693, 1813, 3439, 1959, 1247, 558, 847, 3079, ...  \n",
       "6037  [3327, 908, 1192, 2077, 1366, 352, 1063, 1132,...  \n",
       "6038  [109, 279, 1998, 1211, 918, 3064, 935, 3019, 2...  \n",
       "6039  [847, 2315, 589, 1950, 1892, 3042, 211, 3436, ...  \n",
       "\n",
       "[6040 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from replay.data.nn.utils import groupby_sequences\n",
    "\n",
    "\n",
    "def bake_data(full_data):\n",
    "    grouped_interactions = groupby_sequences(events=full_data, groupby_col=\"user_id\", sort_col=\"timestamp\")\n",
    "    return grouped_interactions\n",
    "\n",
    "\n",
    "train_events = bake_data(train_events)\n",
    "\n",
    "validation_events = bake_data(validation_events)\n",
    "validation_gt = bake_data(validation_gt)\n",
    "\n",
    "test_events = bake_data(test_events)\n",
    "test_gt = bake_data(test_gt)\n",
    "\n",
    "train_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we don't have unknown users in ground truth, we join validation events and validation ground truth (also join test events and test ground truth correspondingly) by user ids to leave only the common ones.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gt_to_events(events_df, gt_df):\n",
    "    gt_to_join = gt_df[[\"user_id\", \"item_id\"]].rename(columns={\"item_id\": \"ground_truth\"})\n",
    "\n",
    "    events_df = events_df.merge(gt_to_join, on=\"user_id\", how=\"inner\")\n",
    "    return events_df\n",
    "\n",
    "validation_events = add_gt_to_events(validation_events, validation_gt)\n",
    "test_events = add_gt_to_events(test_events, test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"temp/data/\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_PATH = data_dir / \"train.parquet\"\n",
    "VAL_PATH = data_dir / \"val.parquet\"\n",
    "TEST_PATH = data_dir / \"test.parquet\"\n",
    "\n",
    "PATH_ENCODED_FEATURES = data_dir / \"item_features_encoded.parquet\"\n",
    "\n",
    "ENCODER_PATH = data_dir / \"encoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_events.to_parquet(TRAIN_PATH)\n",
    "validation_events.to_parquet(VAL_PATH)\n",
    "test_events.to_parquet(TEST_PATH)\n",
    "\n",
    "item_features_encoded[[\"item_id\"]].to_parquet(PATH_ENCODED_FEATURES)\n",
    "\n",
    "encoder.save(ENCODER_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare to model training\n",
    "### Create the tensor schema\n",
    "A schema shows the correspondence of columns from the source dataset with the internal representation of tensors inside the model. It is required by the NN models to correctly create embeddings for every source column. Note that user_id does not required in `TensorSchema`.\n",
    "\n",
    "Note that **cardinality** is the number of unique values â€‹in the item catalog (vocabulary). **Padding value** is the next value after the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.data import FeatureHint, FeatureType, FeatureSource\n",
    "from replay.data.nn import TensorFeatureInfo, TensorFeatureSource, TensorSchema\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "NUM_UNIQUE_ITEMS = len(encoder.mapping[\"item_id\"])\n",
    "\n",
    "tensor_schema = TensorSchema(\n",
    "    [\n",
    "        TensorFeatureInfo(\n",
    "            name=\"item_id\",\n",
    "            is_seq=True,\n",
    "            padding_value=NUM_UNIQUE_ITEMS,\n",
    "            cardinality=NUM_UNIQUE_ITEMS,\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            feature_type=FeatureType.CATEGORICAL,\n",
    "            feature_hint=FeatureHint.ITEM_ID,\n",
    "            feature_sources=[TensorFeatureSource(FeatureSource.INTERACTIONS, \"item_id\")]\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure ParquetModule and transformation pipelines\n",
    "\n",
    "The `ParquetModule` class enables training of models on large datasets by reading data in batch-wise way. This class initialized with **paths to every data split, a metadata dict containing information about shape and padding value of every column and a dict of transforms**. `ParquetModule`'s  \"transform pipelines\" are stage-specific modules implementing additional preprocessing to be performed on batch level right before the forward pass.  \n",
    "\n",
    "For SasRec model (User tower), RePlay provides a function that generates a sequence of appropriate transforms for each data split named **make_default_sasrec_transforms**.\n",
    "\n",
    "Internally this function creates the following transforms:\n",
    "1) Training:\n",
    "    1. Create a target, which contains the shifted item sequence that represents the next item in the sequence (for the next item prediction task).\n",
    "    2. Rename features to match it with expected format by the model during training.\n",
    "    3. Unsqueeze target (*positive_labels*) and it's padding mask (*target_padding_mask*) for getting required shape of this tensors for loss computation.\n",
    "    4. Group input features to be embed in expected format.\n",
    "\n",
    "2) Validation/Inference:\n",
    "    1. Rename/group features to match it with expected format by the model during valdiation/inference.\n",
    "\n",
    "If a different set of transforms is required, you can create them yourself and submit them to the ParquetModule in the form of a dictionary where the key is the name of the split, and the value is the list of transforms. Available transforms are in the replay/nn/transforms/.\n",
    "\n",
    "**Note:** One of the transforms for the training data prepares the initial sequence for the task of Next Item Prediction so it shifts the sequence of items. For the final sequence length to be correct, you need to set shape of item_id in metadata as **model sequence length + shift**. Default shift value is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.nn.transform.template import make_default_twotower_transforms\n",
    "\n",
    "transforms = make_default_twotower_transforms(tensor_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 50\n",
    "\n",
    "train_metadata = {\n",
    "    \"train\": {\n",
    "        \"item_id\": {\"shape\": MAX_SEQ_LEN + 1, \"padding\": tensor_schema[\"item_id\"].padding_value},\n",
    "    },\n",
    "    \"validate\": {\n",
    "        \"item_id\": {\"shape\": MAX_SEQ_LEN, \"padding\": tensor_schema[\"item_id\"].padding_value},\n",
    "        \"ground_truth\": {\"shape\": 1, \"padding\": -1}\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18423/1761019238.py:5: UserWarning: The following dataset paths aren't provided: test,predict. Make sure to disable these stages in your Lightning Trainer configuration.\n",
      "  parquet_module = ParquetModule(\n"
     ]
    }
   ],
   "source": [
    "from replay.data.nn import ParquetModule\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "parquet_module = ParquetModule(\n",
    "    train_path=TRAIN_PATH,\n",
    "    validate_path=VAL_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    metadata=train_metadata,\n",
    "    transforms=transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "### Create TwoTower model instance and run the training stage using lightning\n",
    "We may now train the model using the Lightning trainer class. \n",
    "\n",
    "RePlay's implementation of TwoTower is designed in a modular, **block-based approach**. Instead of passing configuration parameters to the constructor, TwoTower is built by providing fully initialized components that makes the model more flexible and easier to extend. \n",
    "\n",
    "\n",
    "#### Default Configuration\n",
    "\n",
    "Default TwoTower model may be created quickly via method *from_params*. Default model instance has CE loss, user tower is SasRec with original SasRec transformer layes and sum aggregated embeddings, item tower is a SwiGlU MLP block. Both towers use the same features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.nn.sequential import TwoTower\n",
    "from replay.nn.sequential.twotower import FeaturesReader\n",
    "\n",
    "NUM_BLOCKS = 2\n",
    "NUM_HEADS = 2\n",
    "DROPOUT = 0.3\n",
    "\n",
    "twotower = TwoTower.from_params(\n",
    "    schema=tensor_schema,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_sequence_length=MAX_SEQ_LEN,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    dropout=DROPOUT,\n",
    "    item_features_reader=FeaturesReader(\n",
    "        schema=tensor_schema,\n",
    "        metadata={\"item_id\": {}},\n",
    "        path=PATH_ENCODED_FEATURES,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A universal PyTorch Lightning module is provided that can work with any NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.nn.lightning import LightningModule\n",
    "\n",
    "model = LightningModule(twotower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate training, we add the following callbacks:\n",
    "1) `ModelCheckpoint` - to save the best trained model based on its Recall metric. It's a default Lightning Callback.\n",
    "1) `ComputeMetricsCallback` - to display a detailed validation metric matrix after each epoch. It's a custom RePlay callback for computing recsys metrics on validation and test stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type     | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model | TwoTower | 352 K  | train\n",
      "-------------------------------------------\n",
      "352 K     Trainable params\n",
      "0         Non-trainable params\n",
      "352 K     Total params\n",
      "1.409     Total estimated model params size (MB)\n",
      "53        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f461751d40394a6aa1870f6335907dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f8bfbc860d465895509fa11e25fe99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c122be60ace4a328e4e8ef1f48712fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 189: 'recall@10' reached 0.03460 (best 0.03460), saving model to '/home/RePlay/examples/twotower/checkpoints/epoch=0-step=189.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k             1         5         10        20\n",
      "map     0.002649  0.007290  0.009663  0.011498\n",
      "ndcg    0.002649  0.009604  0.015372  0.022204\n",
      "recall  0.002649  0.016722  0.034603  0.061921 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f70e7af4c141c8844094255ed8c8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 378: 'recall@10' reached 0.09801 (best 0.09801), saving model to '/home/RePlay/examples/twotower/checkpoints/epoch=1-step=378.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k             1         5         10        20\n",
      "map     0.011921  0.025138  0.031112  0.035318\n",
      "ndcg    0.011921  0.031891  0.046480  0.061913\n",
      "recall  0.011921  0.052649  0.098013  0.159272 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b4f38c9e0b42fe9e6e01afc38e76cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 567: 'recall@10' reached 0.13046 (best 0.13046), saving model to '/home/RePlay/examples/twotower/checkpoints/epoch=2-step=567.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k             1         5         10        20\n",
      "map     0.013576  0.034222  0.041307  0.047246\n",
      "ndcg    0.013576  0.044604  0.061936  0.083906\n",
      "recall  0.013576  0.076490  0.130464  0.218046 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4413b1e6adeb4d9eb4bc612c5f727a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 756: 'recall@10' reached 0.14950 (best 0.14950), saving model to '/home/RePlay/examples/twotower/checkpoints/epoch=3-step=756.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k            1         5         10        20\n",
      "map     0.01606  0.040792  0.048443  0.055003\n",
      "ndcg    0.01606  0.053037  0.071881  0.095889\n",
      "recall  0.01606  0.090563  0.149503  0.244702 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989f8b0f12bd4ecfa31c19de83290897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 945: 'recall@10' reached 0.16887 (best 0.16887), saving model to '/home/RePlay/examples/twotower/checkpoints/epoch=4-step=945.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k             1         5         10        20\n",
      "map     0.019205  0.045919  0.055054  0.061132\n",
      "ndcg    0.019205  0.059209  0.081423  0.103788\n",
      "recall  0.019205  0.100000  0.168874  0.257781 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from replay.nn.lightning.callback import ComputeMetricsCallback\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"twotower/checkpoints/\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"recall@10\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "validation_metrics_callback = ComputeMetricsCallback(\n",
    "    metrics=[\"map\", \"ndcg\", \"recall\"],\n",
    "    ks=[1, 5, 10, 20],\n",
    "    item_count=NUM_UNIQUE_ITEMS,\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\"twotower/.logs/train\", name=\"TwoTower-example\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint_callback, validation_metrics_callback],\n",
    "    logger=csv_logger,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=parquet_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the best model path stored in the checkpoint callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/RePlay/examples/twotower/checkpoints/epoch=4-step=945.ckpt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path = checkpoint_callback.best_model_path\n",
    "best_model_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "To obtain model scores, we will load the weights from the best checkpoint. To do this, we use the `LightningModule`, provide there the path to the checkpoint and the model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "twotower = TwoTower.from_params(\n",
    "    schema=tensor_schema,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_sequence_length=MAX_SEQ_LEN,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    dropout=DROPOUT,\n",
    "    item_features_reader=FeaturesReader(\n",
    "        schema=tensor_schema,\n",
    "        metadata={\"item_id\": {}},\n",
    "        path=PATH_ENCODED_FEATURES,\n",
    "    )\n",
    ")\n",
    "\n",
    "best_model = LightningModule.load_from_checkpoint(best_model_path, model=twotower)\n",
    "best_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure `ParquetModule` for inference. It's necessary to read \"user_id\" column from data for correctly mapping predictions, so add \"user_id\" to metadata for `ParquetModule`. For reading non-array column dict should be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_metadata = {\n",
    "    \"predict\": {\n",
    "        \"user_id\": {},\n",
    "        \"item_id\": {\"shape\": MAX_SEQ_LEN, \"padding\": tensor_schema[\"item_id\"].padding_value},\n",
    "    }\n",
    "}\n",
    "\n",
    "parquet_module = ParquetModule(\n",
    "    predict_path=TEST_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    metadata=inference_metadata,\n",
    "    transforms=transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During inference, we can use `TopItemsCallback`. Such callback allows you to get scores for each user throughout the entire catalog and get recommendations in the form of ids of items with the highest score values.\n",
    "\n",
    "\n",
    "Recommendations can be fetched in four formats: PySpark DataFrame, Pandas DataFrame, Polars DataFrame or raw PyTorch tensors. Each of the types corresponds a callback. In this example, we'll be using the `PandasTopItemsCallback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465ca19c516a48109a11b512ffd04afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from replay.nn.lightning.callback import PandasTopItemsCallback\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\"twotower/.logs/test\", name=\"TwoTower-example\")\n",
    "\n",
    "TOPK = [1, 5, 10, 20]\n",
    "\n",
    "pandas_prediction_callback = PandasTopItemsCallback(\n",
    "    top_k=max(TOPK),\n",
    "    query_column=\"user_id\",\n",
    "    item_column=\"item_id\",\n",
    "    rating_column=\"score\",\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(callbacks=[pandas_prediction_callback], logger=csv_logger, inference_mode=True)\n",
    "trainer.predict(best_model, datamodule=parquet_module, return_predictions=False)\n",
    "\n",
    "pandas_res = pandas_prediction_callback.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>28.124413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>773</td>\n",
       "      <td>27.929251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1526</td>\n",
       "      <td>27.875792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>584</td>\n",
       "      <td>27.831373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1838</td>\n",
       "      <td>27.78071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6039</td>\n",
       "      <td>504</td>\n",
       "      <td>29.747719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6039</td>\n",
       "      <td>1079</td>\n",
       "      <td>29.701241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6039</td>\n",
       "      <td>297</td>\n",
       "      <td>29.686306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6039</td>\n",
       "      <td>333</td>\n",
       "      <td>29.681482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6039</td>\n",
       "      <td>352</td>\n",
       "      <td>29.658636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120760 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id item_id      score\n",
       "0           0     360  28.124413\n",
       "0           0     773  27.929251\n",
       "0           0    1526  27.875792\n",
       "0           0     584  27.831373\n",
       "0           0    1838   27.78071\n",
       "...       ...     ...        ...\n",
       "6037     6039     504  29.747719\n",
       "6037     6039    1079  29.701241\n",
       "6037     6039     297  29.686306\n",
       "6037     6039     333  29.681482\n",
       "6037     6039     352  29.658636\n",
       "\n",
       "[120760 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating metrics\n",
    "\n",
    "*test_gt* is already encoded, so we can use it for computing metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.metrics import MAP, OfflineMetrics, Precision, Recall\n",
    "from replay.metrics.torch_metrics_builder import metrics_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_metrics = OfflineMetrics(\n",
    "    [Recall(TOPK), Precision(TOPK), MAP(TOPK)],\n",
    "    query_column=\"user_id\",\n",
    "    rating_column=\"score\",\n",
    ")(pandas_res, test_gt.explode(\"item_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>k</th>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAP</th>\n",
       "      <td>0.016893</td>\n",
       "      <td>0.042296</td>\n",
       "      <td>0.050793</td>\n",
       "      <td>0.057049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.016893</td>\n",
       "      <td>0.018417</td>\n",
       "      <td>0.015750</td>\n",
       "      <td>0.012405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.016893</td>\n",
       "      <td>0.092083</td>\n",
       "      <td>0.157502</td>\n",
       "      <td>0.248095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "k                1         5         10        20\n",
       "MAP        0.016893  0.042296  0.050793  0.057049\n",
       "Precision  0.016893  0.018417  0.015750  0.012405\n",
       "Recall     0.016893  0.092083  0.157502  0.248095"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_to_df(result_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the `inverse_transform` encoder's function to get the final dataframe with recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>364</td>\n",
       "      <td>28.124413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>783</td>\n",
       "      <td>27.929251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1566</td>\n",
       "      <td>27.875792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>588</td>\n",
       "      <td>27.831373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1907</td>\n",
       "      <td>27.78071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6040</td>\n",
       "      <td>508</td>\n",
       "      <td>29.747719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6040</td>\n",
       "      <td>1095</td>\n",
       "      <td>29.701241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6040</td>\n",
       "      <td>300</td>\n",
       "      <td>29.686306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6040</td>\n",
       "      <td>337</td>\n",
       "      <td>29.681482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6040</td>\n",
       "      <td>356</td>\n",
       "      <td>29.658636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120760 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  item_id      score\n",
       "0           1      364  28.124413\n",
       "0           1      783  27.929251\n",
       "0           1     1566  27.875792\n",
       "0           1      588  27.831373\n",
       "0           1     1907   27.78071\n",
       "...       ...      ...        ...\n",
       "6037     6040      508  29.747719\n",
       "6037     6040     1095  29.701241\n",
       "6037     6040      300  29.686306\n",
       "6037     6040      337  29.681482\n",
       "6037     6040      356  29.658636\n",
       "\n",
       "[120760 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.inverse_transform(pandas_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp-replay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
