{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RePlay recommender models comparison\n",
    "\n",
    "We will show the main RePlay functionality and compare performance of RePlay models on well-known MovieLens dataset. If you have not used RePlay before, start with 01_replay_basics.ipynb which introduces base concepts and describe main classes and functionality.\n",
    "\n",
    "### Dataset\n",
    "We will compare RePlay models on __MovieLens 1m__. \n",
    "\n",
    "### Dataset preprocessing: \n",
    "Ratings greater than or equal to 3 are considered as positive interactions.\n",
    "\n",
    "### Data split\n",
    "Dataset is split by date so that 20% of the last interactions as are placed in the test part. Cold items and users are dropped.\n",
    "\n",
    "### Predict:\n",
    "We will predict top-10 most relevant films for each user.\n",
    "\n",
    "### Metrics\n",
    "Quality metrics used:__ndcg@k, hitrate@k, map@k, mrr@k__ for k = 1, 5, 10\n",
    "Additional metrics used: __coverage@k__ and __surprisal@k__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q rs-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T16:01:45.639135Z",
     "start_time": "2020-02-10T16:01:45.612577Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from optuna.exceptions import ExperimentalWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ExperimentalWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "from pyspark.sql import functions as sf, types as st\n",
    "from pyspark.sql.types import IntegerType\n",
    "from rs_datasets import MovieLens\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from replay.experimental.models import ULinUCB, HierarchicalRecommender\n",
    "from replay.experimental.models.base_rec import HybridRecommender\n",
    "from replay.experimental.preprocessing.data_preparator import DataPreparator, Indexer\n",
    "from replay.metrics import Experiment\n",
    "from replay.metrics import Coverage, HitRate, MRR, MAP, NDCG, Surprisal\n",
    "from replay.models import PopRec, RandomRec, UCB, Wilson\n",
    "from replay.utils.session_handler import State\n",
    "from replay.splitters import TimeSplitter\n",
    "from replay.utils.spark_utils import get_log_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`State` object allows passing existing Spark session or create a new one, which will be used by the all RePlay modules.\n",
    "\n",
    "To create session with custom parameters ``spark.driver.memory`` and ``spark.sql.shuffle.partitions`` use function `get_spark_session` from `session_handler` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T15:59:09.227179Z",
     "start_time": "2020-02-10T15:59:06.427348Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/29 18:25:45 WARN Utils: Your hostname, UX430 resolves to a loopback address: 127.0.1.1; using 192.168.1.135 instead (on interface wlp2s0)\n",
      "23/08/29 18:25:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/arqa/SbRePlay/RePlay/.venv/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/08/29 18:25:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "23/08/29 18:25:46 WARN DependencyUtils: Local jar /home/arqa/SbRePlay/RePlay/experiments/jars/replay_2.12-0.1_spark_3.1.jar does not exist, skipping.\n",
      "23/08/29 18:25:47 INFO SparkContext: Running Spark version 3.1.3\n",
      "23/08/29 18:25:47 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/08/29 18:25:47 INFO ResourceUtils: ==============================================================\n",
      "23/08/29 18:25:47 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/08/29 18:25:47 INFO ResourceUtils: ==============================================================\n",
      "23/08/29 18:25:47 INFO SparkContext: Submitted application: pyspark-shell\n",
      "23/08/29 18:25:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/08/29 18:25:47 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/08/29 18:25:47 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/08/29 18:25:47 INFO SecurityManager: Changing view acls to: arqa\n",
      "23/08/29 18:25:47 INFO SecurityManager: Changing modify acls to: arqa\n",
      "23/08/29 18:25:47 INFO SecurityManager: Changing view acls groups to: \n",
      "23/08/29 18:25:47 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/08/29 18:25:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(arqa); groups with view permissions: Set(); users  with modify permissions: Set(arqa); groups with modify permissions: Set()\n",
      "23/08/29 18:25:47 INFO Utils: Successfully started service 'sparkDriver' on port 35527.\n",
      "23/08/29 18:25:47 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/08/29 18:25:47 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/08/29 18:25:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/08/29 18:25:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/08/29 18:25:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/08/29 18:25:47 INFO DiskBlockManager: Created local directory at /home/arqa/tmp/blockmgr-3557a203-f831-43a8-8675-d6362c6dff93\n",
      "23/08/29 18:25:47 INFO MemoryStore: MemoryStore started with capacity 2.8 GiB\n",
      "23/08/29 18:25:47 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/08/29 18:25:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/08/29 18:25:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://localhost:4040\n",
      "23/08/29 18:25:48 ERROR SparkContext: Failed to add jars/replay_2.12-0.1_spark_3.1.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /home/arqa/SbRePlay/RePlay/experiments/jars/replay_2.12-0.1_spark_3.1.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1929)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1983)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:501)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:501)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:501)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/29 18:25:48 INFO Executor: Starting executor ID driver on host 192.168.1.135\n",
      "23/08/29 18:25:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35585.\n",
      "23/08/29 18:25:48 INFO NettyBlockTransferService: Server created on localhost:35585\n",
      "23/08/29 18:25:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/08/29 18:25:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 35585, None)\n",
      "23/08/29 18:25:48 INFO BlockManagerMasterEndpoint: Registering block manager localhost:35585 with 2.8 GiB RAM, BlockManagerId(driver, localhost, 35585, None)\n",
      "23/08/29 18:25:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 35585, None)\n",
      "23/08/29 18:25:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 35585, None)\n",
      "23/08/29 18:25:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/arqa/SbRePlay/RePlay/experiments/spark-warehouse').\n",
      "23/08/29 18:25:49 INFO SharedState: Warehouse path is 'file:/home/arqa/SbRePlay/RePlay/experiments/spark-warehouse'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f97c0215040>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = State().session\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T15:59:09.227179Z",
     "start_time": "2020-02-10T15:59:06.427348Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/07 13:50:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/07 13:50:43 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "24/11/07 13:50:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/07 13:50:44 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/11/07 13:50:44 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/11/07 13:50:44 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff9755fa8c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = State().session\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"replay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "K_list_metrics = [1, 5, 10]\n",
    "BUDGET = 20\n",
    "BUDGET_NN = 10\n",
    "SEED = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preprocessing <a name='data-preparator'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T15:59:42.041251Z",
     "start_time": "2020-02-10T15:59:09.230636Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0        1     1193       5  978300760\n",
       "1        1      661       3  978302109\n",
       "2        1      914       3  978301968"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "users\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id gender  age  occupation zip_code\n",
       "0        1      F    1          10    48067\n",
       "1        2      M   56          16    70072\n",
       "2        3      M   25          15    55117"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "items\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id                    title                        genres\n",
       "0        1         Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2           Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3  Grumpier Old Men (1995)                Comedy|Romance"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = MovieLens(\"1m\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log preprocessing\n",
    "\n",
    "- converting to spark dataframe\n",
    "- renaming columns\n",
    "- checking for nulls\n",
    "- converting timestamp to Timestamp format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparator = DataPreparator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-Nov-24 13:51:09, replay, INFO: Columns with ids of users or items are present in mapping. The dataframe will be treated as an interactions log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 79.6 ms, sys: 22.6 ms, total: 102 ms\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "log = preparator.transform(columns_mapping={'user_id': 'user_id',\n",
    "                                      'item_id': 'item_id',\n",
    "                                      'relevance': 'rating',\n",
    "                                      'timestamp': 'timestamp'\n",
    "                                     }, \n",
    "                           data=data.ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+-------------------+\n",
      "|user_id|item_id|relevance|          timestamp|\n",
      "+-------+-------+---------+-------------------+\n",
      "|      1|   1193|      5.0|2000-12-31 22:12:40|\n",
      "|      1|    661|      3.0|2000-12-31 22:35:09|\n",
      "+-------+-------+---------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "836478"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will consider ratings >= 3 as positive feedback. A positive feedback is treated with relevance = 1\n",
    "only_positives_log = log.filter(sf.col('relevance') >= 3).withColumn('relevance', sf.lit(1))\n",
    "only_positives_log.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use only algorithms which do not require user and item features and thus set feature dataframes to None\n",
    "user_features=None\n",
    "item_features=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='indexing'></a>\n",
    "### 0.2. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert given users' and items' identifiers (\\_id) to integers starting at zero without gaps (\\_idx) with Indexer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = Indexer(user_col='user_id', item_col='item_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take all available user and item ids from log and features and pass them to Indexer. The _ids_ could repeat, the indexes will be ordered by label frequencies, so the most frequent label gets index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=========================>                             (13 + 15) / 28]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.1 ms, sys: 14.1 ms, total: 46.2 ms\n",
      "Wall time: 48 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "indexer.fit(users=log.select('user_id'),\n",
    "           items=log.select('item_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+-------------------+\n",
      "|user_idx|item_idx|relevance|          timestamp|\n",
      "+--------+--------+---------+-------------------+\n",
      "|    4131|      43|        1|2000-12-31 22:12:40|\n",
      "|    4131|     585|        1|2000-12-31 22:35:09|\n",
      "+--------+--------+---------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 53.4 ms, sys: 8.98 ms, total: 62.4 ms\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "log_replay = indexer.transform(df=only_positives_log)\n",
    "log_replay.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T15:59:50.986401Z",
     "start_time": "2020-02-10T15:59:42.042998Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train info:\n",
      " total lines: 669181, total users: 5397, total items: 3569\n",
      "test info:\n",
      " total lines: 86542, total users: 1139, total items: 3279\n"
     ]
    }
   ],
   "source": [
    "# train/test split \n",
    "train_spl = TimeSplitter(\n",
    "    time_threshold=0.2,\n",
    "    drop_cold_items=True,\n",
    "    drop_cold_users=True,\n",
    "    query_column=\"user_idx\",\n",
    "    item_column=\"item_idx\",\n",
    ")\n",
    "train, test = train_spl.split(log_replay)\n",
    "print('train info:\\n', get_log_info(train))\n",
    "print('test info:\\n', get_log_info(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T15:59:50.986401Z",
     "start_time": "2020-02-10T15:59:42.042998Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535343, 24241)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train/test split for hyperparameters selection\n",
    "opt_train, opt_val = train_spl.split(train)\n",
    "opt_train.count(), opt_val.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_train.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "798993"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative feedback will be used for Wilson and UCB models\n",
    "only_negatives_log = indexer.transform(df=log.filter(sf.col('relevance') < 3).withColumn('relevance', sf.lit(0.)))\n",
    "test_start = test.agg(sf.min('timestamp')).collect()[0][0]\n",
    "\n",
    "# train with both positive and negative feedback\n",
    "pos_neg_train=(train\n",
    "              .withColumn('relevance', sf.lit(1.))\n",
    "              .union(only_negatives_log.filter(sf.col('timestamp') < test_start))\n",
    "             )\n",
    "pos_neg_train.cache()\n",
    "pos_neg_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_neg_train.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+-------------------+\n",
      "|user_idx|item_idx|relevance|          timestamp|\n",
      "+--------+--------+---------+-------------------+\n",
      "|     677|    1314|        1|2000-12-02 05:30:12|\n",
      "|     677|    1282|        1|2000-12-02 05:53:52|\n",
      "+--------+--------+---------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Metrics definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment is used for metrics calculation\n",
    "e = Experiment(\n",
    "    [MAP(K), \n",
    "      NDCG(K), \n",
    "      HitRate(K_list_metrics), \n",
    "      Coverage(K),\n",
    "      Surprisal(K),\n",
    "      MRR(K)],\n",
    "    test,\n",
    "    pos_neg_train,\n",
    "    query_column=\"user_idx\", item_column=\"item_idx\", rating_column=\"relevance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def fit_predict_add_res(name, model, experiment, train, suffix=''):\n",
    "    \"\"\"\n",
    "    Run fit_predict for the `model`, measure time on fit_predict and evaluate metrics\n",
    "    \"\"\"\n",
    "    start_time=time.time()\n",
    "    \n",
    "    logs = {'log': train}\n",
    "    predict_params = {'k': K, 'users': test.select('user_idx').distinct()}\n",
    "    \n",
    "    if isinstance(model, (ULinUCB)):\n",
    "        logs['log'] = pos_neg_train\n",
    "\n",
    "    if isinstance(model, HybridRecommender):\n",
    "        logs['item_features'] = item_features\n",
    "        logs['user_features'] = user_features\n",
    "    \n",
    "    predict_params.update(logs)\n",
    "\n",
    "    model.fit(**logs)\n",
    "    fit_time = time.time() - start_time\n",
    "\n",
    "    pred=model.predict(**predict_params)\n",
    "    pred.cache()\n",
    "    pred.count()\n",
    "    predict_time = time.time() - start_time - fit_time\n",
    "\n",
    "    experiment.add_result(name + suffix, pred)\n",
    "    metric_time = time.time() - start_time - fit_time - predict_time\n",
    "    experiment.results.loc[name + suffix, 'fit_time'] = fit_time\n",
    "    experiment.results.loc[name + suffix, 'predict_time'] = predict_time\n",
    "    experiment.results.loc[name + suffix, 'metric_time'] = metric_time\n",
    "    experiment.results.loc[name + suffix, 'full_time'] = (fit_time + \n",
    "                                                          predict_time +\n",
    "                                                          metric_time)\n",
    "    pred.unpersist()\n",
    "    print(experiment.results[['NDCG@{}'.format(K), 'MRR@{}'.format(K), 'Coverage@{}'.format(K), 'fit_time']].sort_values('NDCG@{}'.format(K), ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def full_pipeline(models, experiment, train, suffix='', budget=BUDGET):\n",
    "    \"\"\"\n",
    "    For each model:\n",
    "        -  if required: run hyperparameters search, set best params and save param values to `experiment`\n",
    "        - pass model to `fit_predict_add_res`        \n",
    "    \"\"\"\n",
    "    \n",
    "    for name, [model, params] in models.items():\n",
    "        model.logger.info(msg='{} started'.format(name))\n",
    "        if params != 'no_opt':\n",
    "            model.logger.info(msg='{} optimization started'.format(name))\n",
    "            best_params = model.optimize(opt_train, \n",
    "                                         opt_val, \n",
    "                                         param_borders=params, \n",
    "                                         item_features=item_features,\n",
    "                                         user_features=user_features,\n",
    "                                         k=K, \n",
    "                                         budget=budget)\n",
    "            logger.info(msg='best params for {} are: {}'.format(name, best_params))\n",
    "            model.set_params(**best_params)\n",
    "        \n",
    "        logger.info(msg='{} fit_predict started'.format(name))\n",
    "        fit_predict_add_res(name, model, experiment, train, suffix)\n",
    "        # here we call protected attribute to get all parameters set during model initialization\n",
    "        experiment.results.loc[name + suffix, 'params'] = str(model._init_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Hierarchical contextual bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. features preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-Nov-24 14:11:22, replay, INFO: Column with ids of users or items is absent in mapping. The dataframe will be treated as a users'/items' features dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------------------+\n",
      "|item_idx|           title|              genres|\n",
      "+--------+----------------+--------------------+\n",
      "|      29|Toy Story (1995)|Animation|Childre...|\n",
      "|     393|  Jumanji (1995)|Adventure|Childre...|\n",
      "+--------+----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_features_original = preparator.transform(columns_mapping={'item_id': 'item_id'}, \n",
    "                           data=data.items)\n",
    "item_features = indexer.transform(df=item_features_original)\n",
    "item_features.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|item_idx|year|\n",
      "+--------+----+\n",
      "|      29|1995|\n",
      "|     393|1995|\n",
      "+--------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year = item_features.withColumn('year', sf.substring(sf.col('title'), -5, 4).astype(st.IntegerType())).select('item_idx', 'year')\n",
    "year.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = (\n",
    "    item_features.select(\n",
    "        \"item_idx\",\n",
    "        sf.split(\"genres\", \"\\|\").alias(\"genres\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_list = (\n",
    "    genres.select(sf.explode(\"genres\").alias(\"genre\"))\n",
    "    .distinct().filter('genre <> \"(no genres listed)\"')\n",
    "    .toPandas()[\"genre\"].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Children's\",\n",
       " 'Musical',\n",
       " 'Action',\n",
       " 'Crime',\n",
       " 'Fantasy',\n",
       " 'Adventure',\n",
       " 'Romance',\n",
       " 'War',\n",
       " 'Sci-Fi',\n",
       " 'Drama',\n",
       " 'Comedy',\n",
       " 'Horror',\n",
       " 'Documentary',\n",
       " 'Animation',\n",
       " 'Mystery',\n",
       " 'Thriller',\n",
       " 'Western',\n",
       " 'Film-Noir']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features = genres\n",
    "for genre in genres_list:\n",
    "    item_features = item_features.withColumn(\n",
    "        genre,\n",
    "        sf.array_contains(sf.col(\"genres\"), genre).astype(IntegerType())\n",
    "    )\n",
    "item_features = item_features.drop(\"genres\").cache()\n",
    "item_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features = item_features.join(year, on='item_idx', how='inner')\n",
    "item_features.cache()\n",
    "item_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+-----+-------+---------+-------+---+------+-----+------+------+-----------+---------+-------+--------+-------+---------+----+\n",
      "|item_idx|Children's|Musical|Action|Crime|Fantasy|Adventure|Romance|War|Sci-Fi|Drama|Comedy|Horror|Documentary|Animation|Mystery|Thriller|Western|Film-Noir|year|\n",
      "+--------+----------+-------+------+-----+-------+---------+-------+---+------+-----+------+------+-----------+---------+-------+--------+-------+---------+----+\n",
      "|      29|         1|      0|     0|    0|      0|        0|      0|  0|     0|    0|     1|     0|          0|        1|      0|       0|      0|        0|1995|\n",
      "|     393|         1|      0|     0|    0|      1|        1|      0|  0|     0|    0|     0|     0|          0|        0|      0|       0|      0|        0|1995|\n",
      "+--------+----------+-------+------+-----+-------+---------+-------+---+------+-----+------+------+-----------+---------+-------+--------+-------+---------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_features.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-Nov-24 14:11:46, replay, INFO: Column with ids of users or items is absent in mapping. The dataframe will be treated as a users'/items' features dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|user_idx|\n",
      "+--------+\n",
      "|    4131|\n",
      "|    2364|\n",
      "+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# both user and item features need to be present in Hierarchical Recommender (at least with only item/user indices column)\n",
    "# we would not actually use user_features so leave the table with empty feature columns\n",
    "user_features_original = preparator.transform(columns_mapping={'user_id': 'user_id'}, \n",
    "                           data=data.users)\n",
    "user_features = indexer.transform(df=user_features_original)\n",
    "empty_user_features = user_features.select(\"user_idx\")\n",
    "user_features = empty_user_features\n",
    "user_features.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+------+-----+-------+---------+-------+---+------+-----+------+------+-----------+---------+-------+--------+-------+---------+\n",
      "|item_idx|Children's|Musical|Action|Crime|Fantasy|Adventure|Romance|War|Sci-Fi|Drama|Comedy|Horror|Documentary|Animation|Mystery|Thriller|Western|Film-Noir|\n",
      "+--------+----------+-------+------+-----+-------+---------+-------+---+------+-----+------+------+-----------+---------+-------+--------+-------+---------+\n",
      "|      29|         1|      0|     0|    0|      0|        0|      0|  0|     0|    0|     1|     0|          0|        1|      0|       0|      0|        0|\n",
      "|     393|         1|      0|     0|    0|      1|        1|      0|  0|     0|    0|     0|     0|          0|        0|      0|       0|      0|        0|\n",
      "+--------+----------+-------+------+-----+-------+---------+-------+---+------+-----+------+------+-----------+---------+-------+--------+-------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# disbalance in numerical year feature and the rest one-hot data makes the regression in uLinUCB fit much worse\n",
    "item_features = item_features.drop(\"year\")\n",
    "item_features.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcbs = {\n",
    "    'HCB (depth=1)': [\n",
    "        HierarchicalRecommender(\n",
    "            depth=1, cluster_model=KMeans(n_clusters=5), recommender_class=ULinUCB,\n",
    "            recommender_params={\"alpha\" : -2.0}\n",
    "        ), 'no_opt'\n",
    "    ],\n",
    "    'HCB (depth=2)': [\n",
    "        HierarchicalRecommender(\n",
    "            depth=2, cluster_model=KMeans(n_clusters=5), recommender_class=ULinUCB,\n",
    "            recommender_params={\"alpha\" : -2.0}\n",
    "        ), 'no_opt'\n",
    "    ],\n",
    "    'HCB (depth=3)': [\n",
    "        HierarchicalRecommender(\n",
    "            depth=3, cluster_model=KMeans(n_clusters=5), recommender_class=ULinUCB,\n",
    "            recommender_params={\"alpha\" : -2.0}\n",
    "        ), 'no_opt'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-Nov-24 15:06:37, replay, INFO: HCB (depth=1) started\n",
      "07-Nov-24 15:06:37, replay, INFO: HCB (depth=1) fit_predict started\n",
      "07-Nov-24 15:31:58, replay, INFO: HCB (depth=2) started                         \n",
      "07-Nov-24 15:31:58, replay, INFO: HCB (depth=2) fit_predict started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                NDCG@10    MRR@10  Coverage@10    fit_time\n",
      "HCB (depth=1)  0.036343  0.076012       0.0071  205.793682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrishanov/miniconda3/envs/hierarchical_recommender/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "100%|█████████████████████████████████████████████| 5/5 [01:01<00:00, 12.25s/it]\n",
      "07-Nov-24 15:58:28, replay, INFO: HCB (depth=3) started                         \n",
      "07-Nov-24 15:58:28, replay, INFO: HCB (depth=3) fit_predict started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                NDCG@10    MRR@10  Coverage@10    fit_time\n",
      "HCB (depth=2)  0.152523  0.240873     0.032496  256.318547\n",
      "HCB (depth=1)  0.036343  0.076012     0.007100  205.793682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agrishanov/miniconda3/envs/hierarchical_recommender/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/agrishanov/miniconda3/envs/hierarchical_recommender/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/agrishanov/miniconda3/envs/hierarchical_recommender/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/agrishanov/miniconda3/envs/hierarchical_recommender/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/agrishanov/miniconda3/envs/hierarchical_recommender/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/home/agrishanov/miniconda3/envs/hierarchical_recommender/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:10<00:42, 10.63s/it]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:16<00:23,  7.67s/it]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [00:20<00:12,  6.19s/it]\u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [00:25<00:05,  5.83s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:30<00:00,  6.16s/it]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:45<03:00, 45.22s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:03<00:14,  3.51s/it]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:06<00:08,  2.93s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [00:12<00:03,  3.28s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:15<00:00,  3.18s/it]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [01:07<01:34, 31.44s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:04<00:18,  4.58s/it]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:08<00:12,  4.29s/it]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [00:14<00:10,  5.04s/it]\u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [00:19<00:04,  4.98s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:22<00:00,  4.51s/it]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [01:39<01:04, 32.09s/it]\n",
      "                                                                                \u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:06<00:24,  6.12s/it]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:16<00:26,  8.91s/it]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [00:22<00:14,  7.28s/it]\u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [00:26<00:06,  6.12s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:29<00:00,  5.89s/it]\u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [02:23<00:36, 36.83s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:04<00:18,  4.67s/it]\u001b[A\n",
      " 40%|██████████████████                           | 2/5 [00:10<00:15,  5.08s/it]\u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [00:14<00:09,  4.83s/it]\u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [00:17<00:03,  4.00s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:22<00:00,  4.41s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [02:55<00:00, 35.08s/it]\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                NDCG@10    MRR@10  Coverage@10    fit_time\n",
      "HCB (depth=2)  0.152523  0.240873     0.032496  256.318547\n",
      "HCB (depth=3)  0.152274  0.240668     0.034407  372.256509\n",
      "HCB (depth=1)  0.036343  0.076012     0.007100  205.793682\n",
      "CPU times: user 21min 42s, sys: 20min 58s, total: 42min 41s\n",
      "Wall time: 1h 20min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e = Experiment(\n",
    "    [MAP(K), \n",
    "      NDCG(K), \n",
    "      HitRate(K_list_metrics), \n",
    "      Coverage(K),\n",
    "      Surprisal(K),\n",
    "      MRR(K)],\n",
    "    test,\n",
    "    pos_neg_train,\n",
    "    query_column=\"user_idx\", item_column=\"item_idx\", rating_column=\"relevance\"\n",
    ")\n",
    "full_pipeline(hcbs, e, train, budget=BUDGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAP@10</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>HitRate@1</th>\n",
       "      <th>HitRate@5</th>\n",
       "      <th>HitRate@10</th>\n",
       "      <th>Coverage@10</th>\n",
       "      <th>Surprisal@10</th>\n",
       "      <th>MRR@10</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>predict_time</th>\n",
       "      <th>metric_time</th>\n",
       "      <th>full_time</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HCB (depth=2)</th>\n",
       "      <td>0.081320</td>\n",
       "      <td>0.152523</td>\n",
       "      <td>0.118525</td>\n",
       "      <td>0.397717</td>\n",
       "      <td>0.568042</td>\n",
       "      <td>0.032496</td>\n",
       "      <td>0.182862</td>\n",
       "      <td>0.240873</td>\n",
       "      <td>256.318547</td>\n",
       "      <td>876.468735</td>\n",
       "      <td>457.694904</td>\n",
       "      <td>1590.482186</td>\n",
       "      <td>{'depth': 2, 'cluster_model': KMeans(n_cluster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCB (depth=3)</th>\n",
       "      <td>0.081155</td>\n",
       "      <td>0.152274</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.396839</td>\n",
       "      <td>0.562774</td>\n",
       "      <td>0.034407</td>\n",
       "      <td>0.183635</td>\n",
       "      <td>0.240668</td>\n",
       "      <td>372.256509</td>\n",
       "      <td>879.736982</td>\n",
       "      <td>440.705198</td>\n",
       "      <td>1692.698689</td>\n",
       "      <td>{'depth': 3, 'cluster_model': KMeans(n_cluster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCB (depth=1)</th>\n",
       "      <td>0.014520</td>\n",
       "      <td>0.036343</td>\n",
       "      <td>0.035996</td>\n",
       "      <td>0.105356</td>\n",
       "      <td>0.210711</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.360603</td>\n",
       "      <td>0.076012</td>\n",
       "      <td>205.793682</td>\n",
       "      <td>861.154209</td>\n",
       "      <td>453.383323</td>\n",
       "      <td>1520.331214</td>\n",
       "      <td>{'depth': 1, 'cluster_model': KMeans(n_cluster...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MAP@10   NDCG@10  HitRate@1  HitRate@5  HitRate@10  \\\n",
       "HCB (depth=2)  0.081320  0.152523   0.118525   0.397717    0.568042   \n",
       "HCB (depth=3)  0.081155  0.152274   0.119403   0.396839    0.562774   \n",
       "HCB (depth=1)  0.014520  0.036343   0.035996   0.105356    0.210711   \n",
       "\n",
       "               Coverage@10  Surprisal@10    MRR@10    fit_time  predict_time  \\\n",
       "HCB (depth=2)     0.032496      0.182862  0.240873  256.318547    876.468735   \n",
       "HCB (depth=3)     0.034407      0.183635  0.240668  372.256509    879.736982   \n",
       "HCB (depth=1)     0.007100      0.360603  0.076012  205.793682    861.154209   \n",
       "\n",
       "               metric_time    full_time  \\\n",
       "HCB (depth=2)   457.694904  1590.482186   \n",
       "HCB (depth=3)   440.705198  1692.698689   \n",
       "HCB (depth=1)   453.383323  1520.331214   \n",
       "\n",
       "                                                          params  \n",
       "HCB (depth=2)  {'depth': 2, 'cluster_model': KMeans(n_cluster...  \n",
       "HCB (depth=3)  {'depth': 3, 'cluster_model': KMeans(n_cluster...  \n",
       "HCB (depth=1)  {'depth': 1, 'cluster_model': KMeans(n_cluster...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.results.sort_values('NDCG@10', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hierarchical_recommender]",
   "language": "python",
   "name": "conda-env-hierarchical_recommender-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "name": "movielens_nmf.ipynb",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "null"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
