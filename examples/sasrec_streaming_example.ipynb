{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SasRec training/inference with stream dataset example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and session initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/replay/RePlay/replay/models/optimization/optuna_mixin.py:240: FeatureUnavailableWarning: Optimization feature not enabled - `optuna` package not found. Ensure you have the package installed if you want to use the `optimize()` method in your recommenders.\n",
      "  warnings.warn(feature_warning)\n",
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from replay.metrics.torch_metrics_builder import metrics_to_df\n",
    "from replay.data import (\n",
    "    FeatureHint,\n",
    "    FeatureSource,\n",
    "    FeatureType,\n",
    ")\n",
    "from replay.data.nn import (\n",
    "    TensorFeatureInfo,\n",
    "    TensorFeatureSource,\n",
    "    TensorSchema,\n",
    ")\n",
    "from replay.experimental.nn.sequential.postprocessors import RemoveSeenItems\n",
    "from replay.metrics import MAP, OfflineMetrics, Precision, Recall\n",
    "from replay.models.nn.sequential import SasRec\n",
    "from replay.experimental.nn.sequential.callbacks import (\n",
    "    PandasPredictionCallback,\n",
    "    ValidationMetricsCallback\n",
    ")\n",
    "from replay.models.nn.sequential.sasrec import (\n",
    "    SasRecPredictionBatch,\n",
    "    SasRecTrainingBatch,\n",
    "    SasRecValidationBatch,\n",
    ")\n",
    "from replay.splitters import LastNSplitter, RatioSplitter\n",
    "from replay.utils.session_handler import get_spark_session\n",
    "\n",
    "# Fix seed to ensure reproducibility\n",
    "L.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 09:47:10 WARN Utils: Your hostname, ecs-vagolubenko resolves to a loopback address: 127.0.1.1; using 10.11.10.197 instead (on interface eth0)\n",
      "25/12/18 09:47:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/18 09:47:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/18 09:47:11 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "spark_session = get_spark_session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data\n",
    "In this example, we will be using the MovieLens dataset, namely the 100k subset.  \n",
    "Begin by loading interactions, item features and user features using the created session.\n",
    "\n",
    "---\n",
    "**NOTE**\n",
    "\n",
    "Current implementation of SasRec does not take into account user/item features. As such, they are only used in this example to get complete lists of users and items.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://__token__:****@gitlab.amazmetest.ru/api/v4/projects/359/packages/pypi/simple, https://__token__:****@gitlab.amazmetest.ru/api/v4/projects/315/packages/pypi/simple, https://__token__:****@gitlab.amazmetest.ru/api/v4/projects/277/packages/pypi/simple, https://__token__:****@gitlab.amazmetest.ru/api/v4/projects/381/packages/pypi/simple, https://download.pytorch.org/whl/cu113\n",
      "Requirement already satisfied: rs-datasets in /root/replay/RePlay/.venv/lib/python3.10/site-packages (0.5.1)\n",
      "Requirement already satisfied: datatable in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from rs-datasets) (1.1.0)\n",
      "Requirement already satisfied: pandas in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from rs-datasets) (2.3.3)\n",
      "Requirement already satisfied: gdown in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from rs-datasets) (5.2.0)\n",
      "Requirement already satisfied: pyarrow in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from rs-datasets) (21.0.0)\n",
      "Requirement already satisfied: tqdm in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from rs-datasets) (4.67.1)\n",
      "Requirement already satisfied: xlrd in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from rs-datasets) (2.0.2)\n",
      "Requirement already satisfied: kaggle in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from rs-datasets) (1.7.4.5)\n",
      "Requirement already satisfied: py7zr in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from rs-datasets) (1.0.0)\n",
      "Requirement already satisfied: openpyxl in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from rs-datasets) (3.1.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from gdown->rs-datasets) (4.14.3)\n",
      "Requirement already satisfied: filelock in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from gdown->rs-datasets) (3.14.0)\n",
      "Requirement already satisfied: requests[socks] in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from gdown->rs-datasets) (2.32.5)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from beautifulsoup4->gdown->rs-datasets) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from beautifulsoup4->gdown->rs-datasets) (4.15.0)\n",
      "Requirement already satisfied: bleach in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from kaggle->rs-datasets) (6.3.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from kaggle->rs-datasets) (2025.11.12)\n",
      "Requirement already satisfied: charset-normalizer in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from kaggle->rs-datasets) (3.4.4)\n",
      "Requirement already satisfied: idna in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from kaggle->rs-datasets) (3.11)\n",
      "Requirement already satisfied: protobuf in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from kaggle->rs-datasets) (6.33.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from kaggle->rs-datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from kaggle->rs-datasets) (8.0.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from kaggle->rs-datasets) (80.9.0)\n",
      "Requirement already satisfied: six>=1.10 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from kaggle->rs-datasets) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from kaggle->rs-datasets) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from kaggle->rs-datasets) (2.6.1)\n",
      "Requirement already satisfied: webencodings in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from kaggle->rs-datasets) (0.5.1)\n",
      "Requirement already satisfied: et-xmlfile in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from openpyxl->rs-datasets) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from pandas->rs-datasets) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from pandas->rs-datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from pandas->rs-datasets) (2025.2)\n",
      "Requirement already satisfied: texttable in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from py7zr->rs-datasets) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.20.0 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from py7zr->rs-datasets) (3.23.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from py7zr->rs-datasets) (1.2.0)\n",
      "Requirement already satisfied: psutil in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from py7zr->rs-datasets) (7.0.0)\n",
      "Requirement already satisfied: pyzstd>=0.16.1 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from py7zr->rs-datasets) (0.19.1)\n",
      "Requirement already satisfied: pyppmd<1.3.0,>=1.1.0 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from py7zr->rs-datasets) (1.2.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from py7zr->rs-datasets) (1.0.7)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from py7zr->rs-datasets) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from py7zr->rs-datasets) (1.0.4)\n",
      "Requirement already satisfied: backports-zstd>=1.0.0 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from pyzstd>=0.16.1->py7zr->rs-datasets) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /root/replay/RePlay/.venv/lib/python3.10/site-packages (from requests[socks]->gdown->rs-datasets) (1.7.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rs-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rs_datasets import MovieLens\n",
    "\n",
    "movielens = MovieLens(\"100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=======>                                                 (2 + 14) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+\n",
      "|user_id|item_id|   timestamp|\n",
      "+-------+-------+------------+\n",
      "|      1|      1|874965758001|\n",
      "|      1|      2|876893171002|\n",
      "|      1|      3|878542960003|\n",
      "|      1|      4|876893119004|\n",
      "|      1|      5|889751712005|\n",
      "+-------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "int_win = Window.partitionBy(\"user_id\").orderBy(\"item_id\")\n",
    "interactions = spark_session.createDataFrame(movielens.ratings)\n",
    "\n",
    "# NOTE: The following code block is optional and is used\n",
    "# to counteract the issue of identical timestamps in the dataset.\n",
    "# Uncomment if you wish to use it.\n",
    "interactions = (\n",
    "    interactions.select([\"user_id\", \"item_id\", \"timestamp\"])\n",
    "    .withColumn(\"ts\", F.col(\"timestamp\").cast(\"long\") * 1000)\n",
    "    .withColumn(\"row_num\", F.row_number().over(int_win))\n",
    "    .withColumn(\"timestamp\", (F.col(\"ts\") + F.col(\"row_num\")).cast(\"string\"))\n",
    "    .drop(\"ts\", \"row_num\")\n",
    ")\n",
    "\n",
    "interactions.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+----------+--------+\n",
      "|user_id|gender|age|occupation|zip_code|\n",
      "+-------+------+---+----------+--------+\n",
      "|      1|    24|  M|technician|   85711|\n",
      "|      2|    53|  F|     other|   94043|\n",
      "|      3|    23|  M|    writer|   32067|\n",
      "|      4|    24|  M|technician|   43537|\n",
      "|      5|    33|  F|     other|   15213|\n",
      "+-------+------+---+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_features = spark_session.createDataFrame(movielens.users)\n",
    "user_features.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------+--------------------+-------+------+---------+---------+----------+------+-----+-----------+-----+-------+---------+------+-------+-------+-------+------+--------+-----+-------+\n",
      "|item_id|            title|release_date|            imdb_url|unknown|Action|Adventure|Animation|Children's|Comedy|Crime|Documentary|Drama|Fantasy|Film-Noir|Horror|Musical|Mystery|Romance|Sci-Fi|Thriller|  War|Western|\n",
      "+-------+-----------------+------------+--------------------+-------+------+---------+---------+----------+------+-----+-----------+-----+-------+---------+------+-------+-------+-------+------+--------+-----+-------+\n",
      "|      1| Toy Story (1995)| 01-Jan-1995|http://us.imdb.co...|  false| false|    false|     true|      true|  true|false|      false|false|  false|    false| false|  false|  false|  false| false|   false|false|  false|\n",
      "|      2| GoldenEye (1995)| 01-Jan-1995|http://us.imdb.co...|  false|  true|     true|    false|     false| false|false|      false|false|  false|    false| false|  false|  false|  false| false|    true|false|  false|\n",
      "|      3|Four Rooms (1995)| 01-Jan-1995|http://us.imdb.co...|  false| false|    false|    false|     false| false|false|      false|false|  false|    false| false|  false|  false|  false| false|    true|false|  false|\n",
      "|      4|Get Shorty (1995)| 01-Jan-1995|http://us.imdb.co...|  false|  true|    false|    false|     false|  true|false|      false| true|  false|    false| false|  false|  false|  false| false|   false|false|  false|\n",
      "|      5|   Copycat (1995)| 01-Jan-1995|http://us.imdb.co...|  false| false|    false|    false|     false| false| true|      false| true|  false|    false| false|  false|  false|  false| false|    true|false|  false|\n",
      "+-------+-----------------+------------+--------------------+-------+------+---------+---------+----------+------+-----+-----------+-----+-------+---------+------+-------+-------+-------+------+--------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_features = spark_session.createDataFrame(movielens.items)\n",
    "item_features.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode catagorical data.\n",
    "To ensure all categorical data is fit for training, it needs to be encoded using the `LabelEncoder` class. Create an instance of the encoder, providing a `LabelEncodingRule` for each categorcial column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.preprocessing.label_encoder import LabelEncoder, LabelEncodingRule\n",
    "from replay.utils.types import SparkDataFrame\n",
    "\n",
    "\n",
    "def encode_data(queries: SparkDataFrame, items: SparkDataFrame, interactions: SparkDataFrame, label_encoder: LabelEncoder):\n",
    "    full_data = interactions.join(queries, on=\"user_id\").join(items, on=\"item_id\")\n",
    "    full_data = label_encoder.fit_transform(full_data)\n",
    "\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 09:49:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/12/18 09:49:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/18 09:49:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder([\n",
    "    LabelEncodingRule(\"user_id\", default_value=\"last\"),\n",
    "    LabelEncodingRule(\"item_id\", default_value=\"last\")\n",
    "])\n",
    "encoded_interactions = encode_data(user_features, item_features, interactions, encoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split interactions into the train, validation and test datasets using RatioSplitter\n",
    "\n",
    "In order to facilitate the model's training, we split the dataset in the following way:\n",
    "1) A 60/40 data split of original data for training and subsequent splits\n",
    "2) A 75/25 split of the leftover data for testing/validation respectively (i.e. 30%/10% of the full dataset)\n",
    "\n",
    "We also remove cold users/items after each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_events.count()=59623, test_events.count()=40170\n"
     ]
    }
   ],
   "source": [
    "train_events, test_events = RatioSplitter(\n",
    "    test_size=0.4,\n",
    "    divide_column=\"user_id\",\n",
    "    query_column=\"user_id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    "    drop_cold_users=True,\n",
    "    drop_cold_items=True,\n",
    ").split(encoded_interactions)\n",
    "\n",
    "print(f\"{train_events.count()=}, {test_events.count()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_events.count()=29781, val_events.count()=10327\n"
     ]
    }
   ],
   "source": [
    "test_events, val_events = RatioSplitter(\n",
    "    test_size=0.25,\n",
    "    divide_column=\"user_id\",\n",
    "    query_column=\"user_id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    "    drop_cold_users=True,\n",
    "    drop_cold_items=True,\n",
    ").split(test_events)\n",
    "\n",
    "print(f\"{test_events.count()=}, {val_events.count()=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the validation dataset into events and ground_truth\n",
    "\n",
    "For both validation and testing data, the last N items are split into ground truth, which will be used to calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_events.count()=7534, val_gt.count()=2793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_events.count()=26952, test_gt.count()=2829\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_GROUND_TRUTH_INTERACTIONS_PER_USER = 3\n",
    "TEST_GROUND_TRUTH_INTERACTIONS_PER_USER = 3\n",
    "\n",
    "val_events, val_gt = LastNSplitter(\n",
    "    N=VALIDATION_GROUND_TRUTH_INTERACTIONS_PER_USER, divide_column=\"user_id\", query_column=\"user_id\", strategy=\"interactions\"\n",
    ").split(val_events)\n",
    "print(f\"{val_events.count()=}, {val_gt.count()=}\")\n",
    "\n",
    "test_events, test_gt = LastNSplitter(\n",
    "    N=TEST_GROUND_TRUTH_INTERACTIONS_PER_USER, divide_column=\"user_id\", query_column=\"user_id\", strategy=\"interactions\"\n",
    ").split(test_events)\n",
    "print(f\"{test_events.count()=}, {test_gt.count()=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing (\"baking\")\n",
    "SasRec expects each user in the batch to provide their events in form of a sequence. For this reason, the event splits must be properly processed using the `groupby_sequences` function provided by RePlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.data.nn.utils import groupby_sequences\n",
    "\n",
    "\n",
    "def bake_data(full_data: SparkDataFrame):\n",
    "    grouped_interactions = groupby_sequences(\n",
    "        events=full_data,\n",
    "        groupby_col=\"user_id\",\n",
    "        sort_col=\"timestamp\"\n",
    "    )\n",
    "\n",
    "    return grouped_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/18 09:50:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "train_events = bake_data(train_events)\n",
    "val_events = bake_data(val_events)\n",
    "val_gt = bake_data(val_gt)\n",
    "test_events = bake_data(test_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we don't validate on unknown users, we join train and validation data by user ids, leaving only the common ones.  \n",
    "We also pre-package the validation data with its ground truth and train-time events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Keep common query ids between val_dataset and val_gt\n",
    "val_events = val_events.join(val_gt, on=\"user_id\", how=\"left_semi\")\n",
    "val_gt = val_gt.join(val_events, on=\"user_id\", how=\"left_semi\")\n",
    "\n",
    "gt_to_join = val_gt.select([\"user_id\", \"item_id\"]).withColumnRenamed(\"item_id\", \"ground_truth\")\n",
    "train_to_join = train_events.select([\"user_id\", \"item_id\"]).withColumnRenamed(\"item_id\", \"train\")\n",
    "\n",
    "val_events = val_events.join(gt_to_join, on=\"user_id\", how=\"left\")\n",
    "val_events = val_events.join(train_to_join, on=\"user_id\", how=\"left\")\n",
    "\n",
    "TRAIN_LEN = val_events.select(F.max(F.size(\"train\")).alias(\"res\")).collect()[0].res\n",
    "GT_LEN = val_events.select(F.max(F.size(\"ground_truth\")).alias(\"res\")).collect()[0].res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = \"temp/data/train.parquet\"\n",
    "VAL_PATH = \"temp/data/val.parquet\"\n",
    "TEST_PATH = \"temp/data/test.parquet\"\n",
    "\n",
    "train_events.write.mode(\"overwrite\").parquet(TRAIN_PATH)\n",
    "val_events.write.mode(\"overwrite\").parquet(VAL_PATH)\n",
    "test_events.write.mode(\"overwrite\").parquet(TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the tensor schema\n",
    "A schema shows the correspondence of columns from the source dataset with the internal representation of tensors inside the model. It is requiredby the SasRec model to correctly perform operations such as padding and embeddings aggregation at train time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "\n",
    "ITEM_FEATURE_NAME = \"item_id\"\n",
    "\n",
    "tensor_schema = TensorSchema(\n",
    "    TensorFeatureInfo(\n",
    "        name=\"item_id\",\n",
    "        is_seq=True,\n",
    "        cardinality=len(encoder.mapping[\"item_id\"]),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        feature_type=FeatureType.CATEGORICAL,\n",
    "        feature_sources=[TensorFeatureSource(FeatureSource.INTERACTIONS, \"item_id\")],\n",
    "        feature_hint=FeatureHint.ITEM_ID,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure ParquetModule and transformation pipelines\n",
    "The `ParquetModule` class enables training of models on large datasets by reading data in streaming mode. This class initialized with a metadata dict containing information about dataset's features and miscellanious options for initialization (such as shuffling).\n",
    "\n",
    "Additionally, `ParquetModule` supports \"transform pipelines\" - stage-specific modules implementing additional preprocessing to be performed on batch level right before the forward pass.  \n",
    "\n",
    "In out case, we create the following pipelines:\n",
    "1) Training:\n",
    "    1. Create a label mask, which contains the shifted item sequence that represents the next item in the sequence. This mask is unpadded, meaning we have to fetch an extra item for each sequence and then slice the data appropriately;\n",
    "    2. Rename/group columns to match it with the `NamedTuple` expected by the model during training.\n",
    "    3. Compose columns into the expected `NamedTuple`\n",
    "2) Validation/Inference:\n",
    "    1. Rename/group columns to match it with the `NamedTuple` expected by the model during valdiation/inference.\n",
    "    2. Compose columns into the expected `NamedTuple`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.nn.transforms import (\n",
    "    BatchingTransform,\n",
    "    GroupTransform,\n",
    "    RenameTransform,\n",
    "    NextTokenTransform,\n",
    ")\n",
    "\n",
    "MAX_SEQ_LEN = 50\n",
    "BATCH_SIZE = 128\n",
    "SHIFT=1\n",
    "\n",
    "TRANSFORMS = {\n",
    "    \"train\": [\n",
    "        NextTokenTransform(label_field=\"item_id\", query_features=\"user_id\", shift=SHIFT),\n",
    "        RenameTransform({\"user_id\": \"query_id\", \"item_id_mask\": \"padding_mask\", \"labels_mask\": \"labels_padding_mask\"}),\n",
    "        GroupTransform({\"features\": [\"item_id\"]}),\n",
    "        BatchingTransform(SasRecTrainingBatch)\n",
    "    ],\n",
    "    \"val\": [\n",
    "        RenameTransform({\"user_id\": \"query_id\", \"item_id_mask\": \"padding_mask\"}),\n",
    "        GroupTransform({\"features\": [\"item_id\"]}),\n",
    "        BatchingTransform(SasRecValidationBatch)\n",
    "    ],\n",
    "    \"test\": [\n",
    "        RenameTransform({\"user_id\": \"query_id\", \"item_id_mask\": \"padding_mask\"}),\n",
    "        GroupTransform({\"features\": [\"item_id\"]}),\n",
    "        BatchingTransform(SasRecPredictionBatch)\n",
    "    ]\n",
    "}\n",
    "\n",
    "shared_meta = {\n",
    "    \"user_id\": {},\n",
    "    \"item_id\": {\n",
    "        \"shape\": MAX_SEQ_LEN,\n",
    "        \"padding\": tensor_schema[\"item_id\"].padding_value\n",
    "    }\n",
    "}\n",
    "\n",
    "METADATA = {\n",
    "    \"train\": copy.deepcopy(shared_meta),\n",
    "    \"val\": {\n",
    "        **copy.deepcopy(shared_meta),\n",
    "        \"train\": {\n",
    "            \"shape\": TRAIN_LEN,\n",
    "            \"padding\": -2\n",
    "        },\n",
    "        \"ground_truth\": {\n",
    "            \"shape\": GT_LEN,\n",
    "            \"padding\": -1\n",
    "        },\n",
    "    },\n",
    "    \"test\": copy.deepcopy(shared_meta)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.data.nn import ParquetModule\n",
    "\n",
    "streaming_dataset = ParquetModule(\n",
    "    train_path=TRAIN_PATH,\n",
    "    val_path=VAL_PATH,\n",
    "    test_path=TEST_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    metadata=METADATA,\n",
    "    transforms=TRANSFORMS\n",
    ")\n",
    "\n",
    "# NOTE: You can also create a module specifically for training/inference by providing only their respective datapaths\n",
    "# streaming_dataset_train_only = ParquetModule(\n",
    "#     train_path=TRAIN_PATH,\n",
    "#     val_path=VAL_PATH,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     metadata=METADATA,\n",
    "#     transforms=TRANSFORMS\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "### Create SasRec model instance and run the training stage using lightning\n",
    "We may now train the model using the Lightning trainer class. \n",
    "To facilitate training, we add the following callbacks:\n",
    "1) `ValidationMetricsCallback` - to display a detailed validation metric matrix after each epoch.\n",
    "2) `ModelCheckpoint` - to save the best trained model based on its Recall metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SasRec(tensor_schema, max_seq_len=MAX_SEQ_LEN, dropout_rate=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/root/replay/RePlay/.venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:751: Checkpoint directory /root/replay/RePlay/examples/.checkpoints exists and is not empty.\n",
      "\n",
      "  | Name   | Type             | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | _model | SasRecModel      | 117 K  | train\n",
      "1 | _loss  | CrossEntropyLoss | 0      | train\n",
      "----------------------------------------------------\n",
      "117 K     Trainable params\n",
      "0         Non-trainable params\n",
      "117 K     Total params\n",
      "0.471     Total estimated model params size (MB)\n",
      "35        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b4679dec7741a0903eec8455c5adfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/replay/RePlay/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:123: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k         1        10        20         5\n",
      "map     0.0  0.002707  0.003277  0.002214\n",
      "ndcg    0.0  0.006342  0.009477  0.004601\n",
      "recall  0.0  0.010417  0.019531  0.006510\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/replay/RePlay/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:123: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "/root/replay/RePlay/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0e385c8a7a4471b1911e34491ae064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f24f161b154f70b165f20558da0646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 8: 'recall@10' reached 0.01333 (best 0.01333), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=0-step=8.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.001379  0.003661  0.004645  0.002621\n",
      "ndcg    0.001379  0.008308  0.013262  0.004876\n",
      "recall  0.000460  0.013333  0.027126  0.005977\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a71936c667b414abebd801f78fd6132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 16: 'recall@10' reached 0.01655 (best 0.01655), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=1-step=16.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.002759  0.003970  0.005116  0.002835\n",
      "ndcg    0.002759  0.009617  0.015453  0.005683\n",
      "recall  0.000920  0.016552  0.033563  0.007816\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0715837b8d46078083c1d68e055d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 24: 'recall@10' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.006897  0.005240  0.006886  0.004130\n",
      "ndcg    0.006897  0.010993  0.018811  0.007093\n",
      "recall  0.002299  0.016552  0.039080  0.007816\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a36d9d927fe493e99494c02a1dd1640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 32: 'recall@10' reached 0.01977 (best 0.01977), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=3-step=32.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.011034  0.006542  0.008011  0.005218\n",
      "ndcg    0.011034  0.013396  0.020575  0.008703\n",
      "recall  0.003678  0.019770  0.040460  0.009195\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44894fc57c434ac592e566e8170d256b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 40: 'recall@10' reached 0.02299 (best 0.02299), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=4-step=40.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.008276  0.006884  0.008335  0.005333\n",
      "ndcg    0.008276  0.014907  0.022292  0.009773\n",
      "recall  0.002759  0.022989  0.044138  0.011954\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad5927bf7ad47338270d9a26ca6f33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 48: 'recall@10' reached 0.02943 (best 0.02943), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=5-step=48.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.008276  0.008469  0.010061  0.006713\n",
      "ndcg    0.008276  0.018635  0.026410  0.012663\n",
      "recall  0.002759  0.029425  0.051494  0.016092\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabcae74b58d495ab6dd41f1843099b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 56: 'recall@10' reached 0.03310 (best 0.03310), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=6-step=56.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.009655  0.009902  0.012182  0.007379\n",
      "ndcg    0.009655  0.021121  0.031973  0.013387\n",
      "recall  0.003218  0.033103  0.063908  0.016092\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbf4fdd06844444a3a39ad5cca28df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 64: 'recall@10' reached 0.03816 (best 0.03816), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=7-step=64.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.008276  0.010709  0.012531  0.008077\n",
      "ndcg    0.008276  0.023570  0.031873  0.015128\n",
      "recall  0.002759  0.038161  0.062069  0.019310\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8dd504072240d2b4d6a51f7c01bf74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 72: 'recall@10' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.009655  0.011645  0.013846  0.008966\n",
      "ndcg    0.009655  0.024417  0.034202  0.016141\n",
      "recall  0.003218  0.038161  0.066207  0.019770\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a316a6d3ca5a450da4c03eb7534b837f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 80: 'recall@10' reached 0.04230 (best 0.04230), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=9-step=80.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.012414  0.013926  0.016703  0.010674\n",
      "ndcg    0.012414  0.027765  0.040384  0.018640\n",
      "recall  0.004138  0.042299  0.078621  0.022529\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3536244d5a034a0487fc0c9360f59e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 88: 'recall@10' reached 0.04460 (best 0.04460), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=10-step=88.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.017931  0.015878  0.018796  0.013356\n",
      "ndcg    0.017931  0.030595  0.043104  0.023309\n",
      "recall  0.005977  0.044598  0.080460  0.028506\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095cdf61e8d742a182fed59bfcb96dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 96: 'recall@10' reached 0.04460 (best 0.04460), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=11-step=96.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.016552  0.015271  0.018759  0.012092\n",
      "ndcg    0.016552  0.029896  0.044930  0.020808\n",
      "recall  0.005517  0.044598  0.087356  0.024828\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1e2665be9f4ebaa30f597d96af73d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 104: 'recall@10' reached 0.04782 (best 0.04782), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=12-step=104.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.012414  0.015035  0.018235  0.011195\n",
      "ndcg    0.012414  0.030481  0.044741  0.019434\n",
      "recall  0.004138  0.047816  0.088736  0.023448\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193a5658b1ce4968a77d22c0af162586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 112: 'recall@10' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.016552  0.015766  0.019039  0.012268\n",
      "ndcg    0.016552  0.031083  0.045698  0.021059\n",
      "recall  0.005517  0.046897  0.088736  0.024368\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9078847282cc43f39c8d2cc6b2f38521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 120: 'recall@10' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.015172  0.015242  0.018826  0.011847\n",
      "ndcg    0.015172  0.030643  0.045776  0.020601\n",
      "recall  0.005057  0.046897  0.090115  0.024368\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7c2a24aad4433d9297647f54b5877f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 128: 'recall@10' reached 0.05011 (best 0.05011), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=15-step=128.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.016552  0.015647  0.019261  0.011977\n",
      "ndcg    0.016552  0.032116  0.046968  0.020869\n",
      "recall  0.005517  0.050115  0.092414  0.024828\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035a3032925142fab94b630aa57a6dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 136: 'recall@10' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.019310  0.015955  0.019780  0.012115\n",
      "ndcg    0.019310  0.032391  0.047883  0.020978\n",
      "recall  0.006437  0.049655  0.093793  0.024368\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ee4ae439c04beca426731df787bc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 144: 'recall@10' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.016552  0.015907  0.020235  0.012238\n",
      "ndcg    0.016552  0.032098  0.049018  0.021310\n",
      "recall  0.005517  0.048736  0.097011  0.024828\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0789fef18dcc4952af608c73e766592a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 152: 'recall@10' reached 0.05057 (best 0.05057), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=18-step=152.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.017931  0.016487  0.020537  0.012874\n",
      "ndcg    0.017931  0.033225  0.049172  0.022156\n",
      "recall  0.005977  0.050575  0.096092  0.025747\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3980dcc566ef457ab6d5008a34365662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 160: 'recall@10' reached 0.05149 (best 0.05149), saving model to '/root/replay/RePlay/examples/.checkpoints/epoch=19-step=160.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.020690  0.017529  0.021282  0.013724\n",
      "ndcg    0.020690  0.034374  0.049122  0.022727\n",
      "recall  0.006897  0.051494  0.093333  0.025287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\".checkpoints\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"recall@10\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "validation_metrics_callback = ValidationMetricsCallback(\n",
    "    metrics=[\"map\", \"ndcg\", \"recall\"],\n",
    "    ks=[1, 5, 10, 20],\n",
    "    item_count=len(encoder.mapping[\"item_id\"]),\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/train\", name=\"GPT_example\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=20,\n",
    "    callbacks=[checkpoint_callback, validation_metrics_callback],\n",
    "    logger=csv_logger,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=streaming_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now laod the best model usingthe path stored in the callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = SasRec.load_from_checkpoint(checkpoint_callback.best_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference\n",
    "We can now perform inference using the data module we created earlier. Recommendations can be fetched in four formats: PySpark DataFrame, Pandas DataFrame, Polars DataFrame or raw PyTorch tensors. Each of the types corresponds a callback. Inthis example, we'll be using the `PandasPredictionCallback`.\n",
    "\n",
    "Prediction callbacks can filter results using postprocessors. In our case, we apply the `RemoveSeenItems` postprocessor to filter out items already present in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/root/replay/RePlay/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:123: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6232e626cbcf47f48ec6369ab93a2a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_logger = CSVLogger(save_dir=\".logs/test\", name=\"GPT_example\")\n",
    "\n",
    "TOPK = [1, 2, 3]\n",
    "\n",
    "postprocessors = [\n",
    "    RemoveSeenItems(\n",
    "        seen_path=TEST_PATH,\n",
    "        item_count=tensor_schema[ITEM_FEATURE_NAME].cardinality,\n",
    "        query_column=\"user_id\",\n",
    "        item_column=tensor_schema.item_id_feature_name\n",
    "    )\n",
    "]\n",
    "\n",
    "pandas_prediction_callback = PandasPredictionCallback(\n",
    "    top_k=max(TOPK),\n",
    "    query_column=\"user_id\",\n",
    "    item_column=\"item_id\",\n",
    "    rating_column=\"score\",\n",
    "    postprocessors=postprocessors,\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    callbacks=[pandas_prediction_callback],\n",
    "    logger=csv_logger,\n",
    "    inference_mode=True\n",
    ")\n",
    "best_model.eval()\n",
    "trainer.predict(best_model, datamodule=streaming_dataset, return_predictions=False)\n",
    "\n",
    "pandas_res = pandas_prediction_callback.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>203</td>\n",
       "      <td>3.197812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>215</td>\n",
       "      <td>2.702926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>2.678969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>117</td>\n",
       "      <td>2.713183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>755</td>\n",
       "      <td>2.623668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>923</td>\n",
       "      <td>482</td>\n",
       "      <td>3.044874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>923</td>\n",
       "      <td>173</td>\n",
       "      <td>3.040446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>941</td>\n",
       "      <td>173</td>\n",
       "      <td>3.509326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>941</td>\n",
       "      <td>97</td>\n",
       "      <td>3.448085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>941</td>\n",
       "      <td>55</td>\n",
       "      <td>3.411396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2829 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id item_id     score\n",
       "0          3     203  3.197812\n",
       "0          3     215  2.702926\n",
       "0          3      95  2.678969\n",
       "1         17     117  2.713183\n",
       "1         17     755  2.623668\n",
       "..       ...     ...       ...\n",
       "941      923     482  3.044874\n",
       "941      923     173  3.040446\n",
       "942      941     173  3.509326\n",
       "942      941      97  3.448085\n",
       "942      941      55  3.411396\n",
       "\n",
       "[2829 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to get the recomendations in PySpark format. \n",
    "Let's get the inverse representation of labels using inverse_transform method.\n",
    "\n",
    "Note that the reverse representation can only be obtained for PySpark and Pandas formats. When working with PyTorch tensors, the reverse representation must be done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mapped_gt = encoder.inverse_transform(test_gt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/replay/RePlay/replay/metrics/offline_metrics.py:375: UserWarning: ground_truth contains queries that are not presented in recommendations\n",
      "  warnings.warn(f\"{dataset_name} contains queries that are not presented in recommendations\")\n"
     ]
    }
   ],
   "source": [
    "result_metrics = OfflineMetrics(\n",
    "    [Recall(TOPK), Precision(TOPK), MAP(TOPK)],\n",
    "    query_column=\"user_id\",\n",
    "    rating_column=\"score\"\n",
    ")(pandas_res, test_mapped_gt.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>k</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAP</th>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.002180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.003535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.003535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "k                 1         2         3\n",
       "MAP        0.004242  0.002386  0.002180\n",
       "Precision  0.004242  0.002651  0.003535\n",
       "Recall     0.001414  0.001767  0.003535"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_to_df(result_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "replay-rec-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
