{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SasRec training/inference with stream dataset example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and session initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from replay.metrics.torch_metrics_builder import metrics_to_df\n",
    "from replay.data import (\n",
    "    FeatureHint,\n",
    "    FeatureSource,\n",
    "    FeatureType,\n",
    ")\n",
    "from replay.data.nn import (\n",
    "    TensorFeatureInfo,\n",
    "    TensorFeatureSource,\n",
    "    TensorSchema,\n",
    ")\n",
    "from replay.metrics import MAP, OfflineMetrics, Precision, Recall\n",
    "from replay.splitters import LastNSplitter, RatioSplitter\n",
    "from replay.utils.session_handler import get_spark_session\n",
    "\n",
    "# Fix seed to ensure reproducibility\n",
    "L.seed_everything(42)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/22 12:32:41 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/12/22 12:32:41 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/12/22 12:32:41 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    }
   ],
   "source": [
    "spark_session = get_spark_session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data\n",
    "In this example, we will be using the MovieLens dataset, namely the 100k subset.  \n",
    "Begin by loading interactions, item features and user features using the created session.\n",
    "\n",
    "---\n",
    "**NOTE**\n",
    "\n",
    "Current implementation of SasRec handles only item and interactions features. It does not take into account user features. As such, they are only used in this example to get complete lists of users.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rs-datasets in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (0.5.1)\n",
      "Requirement already satisfied: datatable in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from rs-datasets) (1.1.0)\n",
      "Requirement already satisfied: pandas in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from rs-datasets) (2.3.3)\n",
      "Requirement already satisfied: gdown in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from rs-datasets) (5.2.0)\n",
      "Requirement already satisfied: pyarrow in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from rs-datasets) (21.0.0)\n",
      "Requirement already satisfied: tqdm in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from rs-datasets) (4.67.1)\n",
      "Requirement already satisfied: xlrd in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from rs-datasets) (2.0.2)\n",
      "Requirement already satisfied: kaggle in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from rs-datasets) (1.7.4.5)\n",
      "Requirement already satisfied: py7zr in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from rs-datasets) (1.1.0)\n",
      "Requirement already satisfied: openpyxl in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from rs-datasets) (3.1.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from gdown->rs-datasets) (4.14.3)\n",
      "Requirement already satisfied: filelock in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from gdown->rs-datasets) (3.14.0)\n",
      "Requirement already satisfied: requests[socks] in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from gdown->rs-datasets) (2.32.5)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from beautifulsoup4->gdown->rs-datasets) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from beautifulsoup4->gdown->rs-datasets) (4.15.0)\n",
      "Requirement already satisfied: bleach in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from kaggle->rs-datasets) (6.3.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from kaggle->rs-datasets) (2025.11.12)\n",
      "Requirement already satisfied: charset-normalizer in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from kaggle->rs-datasets) (3.4.4)\n",
      "Requirement already satisfied: idna in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from kaggle->rs-datasets) (3.11)\n",
      "Requirement already satisfied: protobuf in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from kaggle->rs-datasets) (6.33.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from kaggle->rs-datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from kaggle->rs-datasets) (8.0.4)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from kaggle->rs-datasets) (80.9.0)\n",
      "Requirement already satisfied: six>=1.10 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from kaggle->rs-datasets) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from kaggle->rs-datasets) (1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from kaggle->rs-datasets) (2.6.1)\n",
      "Requirement already satisfied: webencodings in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from kaggle->rs-datasets) (0.5.1)\n",
      "Requirement already satisfied: et-xmlfile in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from openpyxl->rs-datasets) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from pandas->rs-datasets) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from pandas->rs-datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from pandas->rs-datasets) (2025.2)\n",
      "Requirement already satisfied: texttable in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from py7zr->rs-datasets) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.20.0 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from py7zr->rs-datasets) (3.23.0)\n",
      "Requirement already satisfied: brotli>=1.2.0 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from py7zr->rs-datasets) (1.2.0)\n",
      "Requirement already satisfied: psutil in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from py7zr->rs-datasets) (7.0.0)\n",
      "Requirement already satisfied: backports.zstd>=1.0.0 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from py7zr->rs-datasets) (1.2.0)\n",
      "Requirement already satisfied: pyppmd>=1.3.1 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from py7zr->rs-datasets) (1.3.1)\n",
      "Requirement already satisfied: pybcj>=1.0.6 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from py7zr->rs-datasets) (1.0.7)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from py7zr->rs-datasets) (0.2.3)\n",
      "Requirement already satisfied: inflate64>=1.0.4 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from py7zr->rs-datasets) (1.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/evtsinovnik/.conda/envs/replay_test_310/lib/python3.10/site-packages (from requests[socks]->gdown->rs-datasets) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install rs-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       ".datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rs_datasets:Downloading ml-100k from grouplens...\n",
      "4.94MB [00:02, 1.82MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "from rs_datasets import MovieLens\n",
    "\n",
    "movielens = MovieLens(\"100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+\n",
      "|user_id|item_id|   timestamp|\n",
      "+-------+-------+------------+\n",
      "|      1|      1|874965758001|\n",
      "|      1|     10|875693118002|\n",
      "|      1|    100|878543541003|\n",
      "|      1|    101|878542845004|\n",
      "|      1|    102|889751736005|\n",
      "+-------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "int_win = Window.partitionBy(\"user_id\").orderBy(\"item_id\")\n",
    "# interactions = spark_session.createDataFrame(movielens.ratings)\n",
    "interactions = spark_session.read.option(\"header\", \"true\").option(\"sep\", \"\\t\").csv(\"tmp/data/ml-100k/ml-100k.inter\")\n",
    "\n",
    "# NOTE: The following code block is optional and is used\n",
    "# to counteract the issue of identical timestamps in the dataset.\n",
    "# Uncomment if you wish to use it.\n",
    "interactions = (\n",
    "    interactions.select([\"user_id\", \"item_id\", \"timestamp\"])\n",
    "    .withColumn(\"ts\", F.col(\"timestamp\").cast(\"long\") * 1000)\n",
    "    .withColumn(\"row_num\", F.row_number().over(int_win))\n",
    "    .withColumn(\"timestamp\", (F.col(\"ts\") + F.col(\"row_num\")).cast(\"string\"))\n",
    "    .drop(\"ts\", \"row_num\")\n",
    ")\n",
    "\n",
    "interactions.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+--------+\n",
      "|user_id|age|gender|occupation|zip_code|\n",
      "+-------+---+------+----------+--------+\n",
      "|      1| 24|     M|technician|   85711|\n",
      "|      2| 53|     F|     other|   94043|\n",
      "|      3| 23|     M|    writer|   32067|\n",
      "|      4| 24|     M|technician|   43537|\n",
      "|      5| 33|     F|     other|   15213|\n",
      "+-------+---+------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# user_features = spark_session.createDataFrame(movielens.users)\n",
    "user_features = spark_session.read.option(\"header\", \"true\").option(\"sep\", \"\\t\").csv(\"tmp/data/ml-100k/ml-100k.user\")\n",
    "user_features.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+--------------------+\n",
      "|item_id|movie_title|release_year|               class|\n",
      "+-------+-----------+------------+--------------------+\n",
      "|      1|  Toy Story|        1995|Animation Childre...|\n",
      "|      2|  GoldenEye|        1995|Action Adventure ...|\n",
      "|      3| Four Rooms|        1995|            Thriller|\n",
      "|      4| Get Shorty|        1995| Action Comedy Drama|\n",
      "|      5|    Copycat|        1995|Crime Drama Thriller|\n",
      "+-------+-----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item_features = spark_session.read.option(\"header\", \"true\").option(\"sep\", \"\\t\").csv(\"tmp/data/ml-100k/ml-100k.item\")\n",
    "item_features.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode catagorical data.\n",
    "To ensure all categorical data is fit for training, it needs to be encoded using the `LabelEncoder` class. Create an instance of the encoder, providing a `LabelEncodingRule` for each categorcial column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.preprocessing.label_encoder import LabelEncoder, LabelEncodingRule\n",
    "from replay.utils.types import SparkDataFrame\n",
    "\n",
    "\n",
    "def encode_data(\n",
    "    queries: SparkDataFrame, items: SparkDataFrame, interactions: SparkDataFrame, label_encoder: LabelEncoder\n",
    "):\n",
    "    full_data = interactions.join(queries, on=\"user_id\").join(items, on=\"item_id\")\n",
    "    full_data = label_encoder.fit_transform(full_data)\n",
    "\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/22 12:32:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:57 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/12/22 12:32:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/22 12:32:58 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder(\n",
    "    [\n",
    "        LabelEncodingRule(\"user_id\", default_value=\"last\"),\n",
    "        LabelEncodingRule(\"item_id\", default_value=\"last\"),\n",
    "        LabelEncodingRule(\"class\", default_value=\"last\"),\n",
    "    ]\n",
    ")\n",
    "encoded_interactions = encode_data(user_features, item_features, interactions, encoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split interactions into the train, validation and test datasets using RatioSplitter\n",
    "\n",
    "In order to facilitate the model's training, we split the dataset in the following way:\n",
    "1) A 60/40 data split of original data for training and subsequent splits\n",
    "2) A 75/25 split of the leftover data for testing/validation respectively (i.e. 30%/10% of the full dataset)\n",
    "\n",
    "We also remove cold users/items after each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_events.count()=59623, test_events.count()=40171\n"
     ]
    }
   ],
   "source": [
    "train_events, test_events = RatioSplitter(\n",
    "    test_size=0.4,\n",
    "    divide_column=\"user_id\",\n",
    "    query_column=\"user_id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    "    drop_cold_users=True,\n",
    "    drop_cold_items=True,\n",
    ").split(encoded_interactions)\n",
    "\n",
    "print(f\"{train_events.count()=}, {test_events.count()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_events.count()=29782, val_events.count()=10330\n"
     ]
    }
   ],
   "source": [
    "test_events, val_events = RatioSplitter(\n",
    "    test_size=0.25,\n",
    "    divide_column=\"user_id\",\n",
    "    query_column=\"user_id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    "    drop_cold_users=True,\n",
    "    drop_cold_items=True,\n",
    ").split(test_events)\n",
    "\n",
    "print(f\"{test_events.count()=}, {val_events.count()=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the validation dataset into events and ground_truth\n",
    "\n",
    "For both validation and testing data, the last N items are split into ground truth, which will be used to calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_events.count()=7535, val_gt.count()=2795\n",
      "test_events.count()=26953, test_gt.count()=2829\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_GROUND_TRUTH_INTERACTIONS_PER_USER = 3\n",
    "TEST_GROUND_TRUTH_INTERACTIONS_PER_USER = 3\n",
    "\n",
    "val_events, val_gt = LastNSplitter(\n",
    "    N=VALIDATION_GROUND_TRUTH_INTERACTIONS_PER_USER,\n",
    "    divide_column=\"user_id\",\n",
    "    query_column=\"user_id\",\n",
    "    strategy=\"interactions\",\n",
    ").split(val_events)\n",
    "print(f\"{val_events.count()=}, {val_gt.count()=}\")\n",
    "\n",
    "test_events, test_gt = LastNSplitter(\n",
    "    N=TEST_GROUND_TRUTH_INTERACTIONS_PER_USER, divide_column=\"user_id\", query_column=\"user_id\", strategy=\"interactions\"\n",
    ").split(test_events)\n",
    "print(f\"{test_events.count()=}, {test_gt.count()=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing (\"baking\")\n",
    "SasRec expects each user in the batch to provide their events in form of a sequence. For this reason, the event splits must be properly processed using the `groupby_sequences` function provided by RePlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.data.nn.utils import groupby_sequences\n",
    "\n",
    "\n",
    "def bake_data(full_data: SparkDataFrame):\n",
    "    grouped_interactions = groupby_sequences(events=full_data, groupby_col=\"user_id\", sort_col=\"timestamp\")\n",
    "\n",
    "    return grouped_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_events = bake_data(train_events)\n",
    "val_events = bake_data(val_events)\n",
    "val_gt = bake_data(val_gt)\n",
    "test_events = bake_data(test_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we don't validate on unknown users, we join train and validation data by user ids, leaving only the common ones.  \n",
    "We also pre-package the validation data with its ground truth and train-time events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Keep common query ids between val_dataset and val_gt\n",
    "val_events = val_events.join(val_gt, on=\"user_id\", how=\"left_semi\")\n",
    "val_gt = val_gt.join(val_events, on=\"user_id\", how=\"left_semi\")\n",
    "\n",
    "gt_to_join = val_gt.select([\"user_id\", \"item_id\"]).withColumnRenamed(\"item_id\", \"ground_truth\")\n",
    "train_to_join = train_events.select([\"user_id\", \"item_id\"]).withColumnRenamed(\"item_id\", \"train\")\n",
    "\n",
    "val_events = val_events.join(gt_to_join, on=\"user_id\", how=\"left\")\n",
    "val_events = val_events.join(train_to_join, on=\"user_id\", how=\"left\")\n",
    "\n",
    "TRAIN_LEN = val_events.select(F.max(F.size(\"train\")).alias(\"res\")).collect()[0].res\n",
    "GT_LEN = val_events.select(F.max(F.size(\"ground_truth\")).alias(\"res\")).collect()[0].res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = \"temp/data/train.parquet\"\n",
    "VAL_PATH = \"temp/data/val.parquet\"\n",
    "TEST_PATH = \"temp/data/test.parquet\"\n",
    "\n",
    "train_events.write.mode(\"overwrite\").parquet(TRAIN_PATH)\n",
    "val_events.write.mode(\"overwrite\").parquet(VAL_PATH)\n",
    "test_events.write.mode(\"overwrite\").parquet(TEST_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the tensor schema\n",
    "A schema shows the correspondence of columns from the source dataset with the internal representation of tensors inside the model. It is required by the SasRec model to correctly create embeddings at train time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "\n",
    "ITEM_FEATURE_NAME = \"item_id\"\n",
    "NUM_UNIQUE_ITEMS = len(encoder.mapping[\"item_id\"])\n",
    "NUM_UNIQUE_CLASS_VALUES = len(encoder.mapping[\"class\"])\n",
    "\n",
    "tensor_schema = TensorSchema(\n",
    "    [\n",
    "        TensorFeatureInfo(\n",
    "            name=\"item_id\",\n",
    "            is_seq=True,\n",
    "            padding_value=NUM_UNIQUE_ITEMS,\n",
    "            cardinality=NUM_UNIQUE_ITEMS + 1,  # taking into account padding\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            feature_type=FeatureType.CATEGORICAL,\n",
    "            feature_sources=[TensorFeatureSource(FeatureSource.ITEM_FEATURES, \"item_id\")],\n",
    "            feature_hint=FeatureHint.ITEM_ID,\n",
    "        ),\n",
    "        TensorFeatureInfo(\n",
    "            name=\"class\",\n",
    "            is_seq=True,\n",
    "            padding_value=NUM_UNIQUE_CLASS_VALUES,\n",
    "            cardinality=NUM_UNIQUE_CLASS_VALUES + 1,  # taking into account padding\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            feature_type=FeatureType.CATEGORICAL,\n",
    "            feature_sources=[TensorFeatureSource(FeatureSource.ITEM_FEATURES, \"item_id\")],\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure ParquetModule and transformation pipelines\n",
    "The `ParquetModule` class enables training of models on large datasets by reading data in streaming mode. This class initialized with a metadata dict containing information about dataset's features and miscellanious options for initialization (such as shuffling).\n",
    "\n",
    "Additionally, `ParquetModule` supports \"transform pipelines\" - stage-specific modules implementing additional preprocessing to be performed on batch level right before the forward pass.  \n",
    "\n",
    "In our case, we create the following pipelines:\n",
    "1) Training:\n",
    "    1. Create a target, which contains the shifted item sequence that represents the next item in the sequence (for the next item prediction task).\n",
    "    2. Optionally sample negatives (required only for sampled losses).\n",
    "    3. Rename features to match it with expected format by the model during training.\n",
    "    4. Unsquueeze target (`positive_labels`) and it's padding mask (`target_padding_mask`).\n",
    "    5. Group input features to be embed in expected format.\n",
    "\n",
    "2) Validation/Inference:\n",
    "    1. Rename/group features to match it with expected format by the model during valdiation/inference.\n",
    "\n",
    "Then, metadata for ParquetModule should be created. It contains shape and padding value for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.nn.transforms import (\n",
    "    UnsqueezeTransform,\n",
    "    GroupTransform,\n",
    "    RenameTransform,\n",
    "    NextTokenTransform,\n",
    "    UniformNegativeSamplingTransform,\n",
    ")\n",
    "\n",
    "MAX_SEQ_LEN = 50\n",
    "BATCH_SIZE = 32\n",
    "SHIFT = 1\n",
    "\n",
    "TRANSFORMS = {\n",
    "    \"train\": [\n",
    "        NextTokenTransform(\n",
    "            label_field=\"item_id\", query_features=\"user_id\", shift=SHIFT, out_feature_name=\"positive_labels\"\n",
    "        ),\n",
    "        UniformNegativeSamplingTransform(vocab_size=NUM_UNIQUE_ITEMS, num_negative_samples=200),\n",
    "        RenameTransform(\n",
    "            {\"user_id\": \"query_id\", \"item_id_mask\": \"padding_mask\", \"positive_labels_mask\": \"target_padding_mask\"}\n",
    "        ),\n",
    "        UnsqueezeTransform(\"target_padding_mask\", -1),\n",
    "        UnsqueezeTransform(\"positive_labels\", -1),\n",
    "        GroupTransform({\"feature_tensors\": [\"item_id\", \"class\"]}),\n",
    "    ],\n",
    "    \"val\": [\n",
    "        RenameTransform({\"user_id\": \"query_id\", \"item_id_mask\": \"padding_mask\"}),\n",
    "        GroupTransform({\"feature_tensors\": [\"item_id\", \"class\"]}),\n",
    "    ],\n",
    "    \"test\": [\n",
    "        RenameTransform({\"user_id\": \"query_id\", \"item_id_mask\": \"padding_mask\"}),\n",
    "        GroupTransform({\"feature_tensors\": [\"item_id\", \"class\"]}),\n",
    "    ],\n",
    "}\n",
    "\n",
    "shared_meta = {\n",
    "    \"user_id\": {},\n",
    "    \"item_id\": {\"shape\": MAX_SEQ_LEN, \"padding\": tensor_schema[\"item_id\"].padding_value},\n",
    "    \"class\": {\"shape\": MAX_SEQ_LEN, \"padding\": tensor_schema[\"class\"].padding_value},\n",
    "}\n",
    "\n",
    "METADATA = {\n",
    "    \"train\": copy.deepcopy(shared_meta),\n",
    "    \"val\": {\n",
    "        **copy.deepcopy(shared_meta),\n",
    "        \"train\": {\n",
    "            \"shape\": TRAIN_LEN, \"padding\": tensor_schema[\"item_id\"].padding_value,\n",
    "        },\n",
    "        \"ground_truth\": {\n",
    "            \"shape\": GT_LEN, \"padding\": tensor_schema[\"item_id\"].padding_value,\n",
    "        },\n",
    "    },\n",
    "    \"test\": copy.deepcopy(shared_meta),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.data.nn import ParquetModule\n",
    "\n",
    "streaming_dataset = ParquetModule(\n",
    "    train_path=TRAIN_PATH,\n",
    "    val_path=VAL_PATH,\n",
    "    test_path=TEST_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    metadata=METADATA,\n",
    "    transforms=TRANSFORMS,\n",
    ")\n",
    "\n",
    "# NOTE: You can also create a module specifically for training/inference by providing only their respective datapaths\n",
    "# streaming_dataset_train_only = ParquetModule(\n",
    "#     train_path=TRAIN_PATH,\n",
    "#     val_path=VAL_PATH,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     metadata=METADATA,\n",
    "#     transforms=TRANSFORMS\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "### Create SasRec model instance and run the training stage using lightning\n",
    "We may now train the model using the Lightning trainer class. \n",
    "\n",
    "RePlay's implementation of SasRec is designed in a modular, **block-based approach**. Instead of passing configuration parameters to the constructor, SasRec is now built by providing fully initialized components that makes the model more flexible and easier to extend. SasRec consists of the following components: embedder, aggregator, encoder, mask, output_normalization, loss.\n",
    "\n",
    "#### Components\n",
    "\n",
    "* `Embedder` -The embedder is responsible for converting input features into embeddings. The default implementation is `SequenceEmbedding`, which supports the following feature types: categorical, categorical_list, numerical, numerical_list\n",
    "\n",
    "* `Aggregator` - The aggregator combines all embeddings produced by the embedder and adds positional embeddings.\n",
    "Currently, `SasRecAggregator` is supported. It internally uses one of the following embedding aggregation strategies: `SumAggregator`, `ConcatAggregator`.\n",
    "\n",
    "* `Encoder` - The encoder represents the core transformer block of the model. The following implementations are currently available: `SasRecTransformerLayer` (default one), `DiffAttentionLayer` (a modified version with differential attention).\n",
    "\n",
    "* `Mask` - The mask is an object that creates attention mask by input. RePlay supports `DefaultAttentionMask` creating a lower-triangular attention mask.\n",
    "\n",
    "* `Output Normalization` - Any suitable PyTorch normalization layer may be used as output_normalization, for example: torch.nn.LayerNorm or torch.nn.RMSNorm\n",
    "\n",
    "* `Loss` - The loss component defines how the training loss is computed. All available loss implementations are located in nn/loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.nn import DefaultAttentionMask, SequenceEmbedding, SumAggregator\n",
    "from replay.nn.loss import CESampled\n",
    "from replay.nn.sequential import SasRec, SasRecAggregator, SasRecTransformerLayer\n",
    "\n",
    "\n",
    "NUM_BLOCKS = 1\n",
    "NUM_HEADS = 1\n",
    "DROPOUT = 0.0\n",
    "\n",
    "sasrec = SasRec(\n",
    "    embedder=SequenceEmbedding(\n",
    "        schema=tensor_schema,\n",
    "        categorical_list_feature_aggregation_method=\"sum\",\n",
    "    ),\n",
    "    embedding_aggregator=SasRecAggregator(\n",
    "        embedding_aggregator=SumAggregator(embedding_dim=EMBEDDING_DIM),\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "        dropout=DROPOUT,\n",
    "    ),\n",
    "    attn_mask_builder=DefaultAttentionMask(\n",
    "        reference_feature_name=tensor_schema.item_id_feature_name,\n",
    "        num_heads=NUM_HEADS,\n",
    "    ),\n",
    "    encoder=SasRecTransformerLayer(\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        num_blocks=NUM_BLOCKS,\n",
    "        dropout=DROPOUT,\n",
    "        activation=\"relu\",\n",
    "    ),\n",
    "    output_normalization=torch.nn.LayerNorm(EMBEDDING_DIM),\n",
    "    loss=CESampled(padding_idx=tensor_schema.item_id_features.item().padding_value),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Configuration\n",
    "\n",
    "Default SasRec model may be created quickly via method build_original. Such model has CE loss, original SasRec transformer layes, and embeddings are aggregated via sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_sasrec = SasRec.build_default(\n",
    "    schema=tensor_schema, \n",
    "    embedding_dim=EMBEDDING_DIM, \n",
    "    max_sequence_length=MAX_SEQ_LEN\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A universal PyTorch Lightning module is provided that can work with any RePlay NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.nn.lightning import LightningModule\n",
    "from replay.models.nn.optimizer_utils import FatOptimizerFactory, FatLRSchedulerFactory\n",
    "\n",
    "model = LightningModule(\n",
    "    sasrec,\n",
    "    optimizer_factory=FatOptimizerFactory(),\n",
    "    lr_scheduler_factory=FatLRSchedulerFactory(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate training, we add the following callbacks:\n",
    "1) `ModelCheckpoint` - to save the best trained model based on its Recall metric. It's a default Lightning Callback.\n",
    "1) `ComputeMetricsCallback` - to display a detailed validation metric matrix after each epoch. It's a custom RePlay callback for computing recsys metrics on validation. It supports model's logits postpocessing (before metrics computing), we will use RePlay `SeenItemsFilter` in order to compute metrics on unseen ground truth items only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type   | Params | Mode \n",
      "-----------------------------------------\n",
      "0 | model | SasRec | 150 K  | train\n",
      "-----------------------------------------\n",
      "150 K     Trainable params\n",
      "0         Non-trainable params\n",
      "150 K     Total params\n",
      "0.601     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19388c86fe5d432ba488bfbebc9949e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d4820a963f44bfb417060d607ffe7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40deedbc01e14ea68a590c95a63ad6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 30: 'recall@10' reached 0.08229 (best 0.08229), saving model to '/home/evtsinovnik/replay_master/examples/.checkpoints/epoch=0-step=30.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.034375  0.029739  0.034592  0.024236\n",
      "ndcg    0.034375  0.055814  0.074573  0.039157\n",
      "recall  0.011458  0.082292  0.136458  0.044792\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d987d7e753749348b3d4ee8993e8f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 60: 'recall@10' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.028125  0.027392  0.033712  0.022448\n",
      "ndcg    0.028125  0.052071  0.074901  0.037400\n",
      "recall  0.009375  0.077083  0.141667  0.044792\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69cf6a9b4074f478abe5e28a9bbbf0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 90: 'recall@10' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.040625  0.030728  0.036701  0.025625\n",
      "ndcg    0.040625  0.057019  0.078737  0.041264\n",
      "recall  0.013542  0.080208  0.141667  0.044792\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62a175e0fff4ab3824ca382dc54dc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 120: 'recall@10' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.025000  0.027705  0.033938  0.021319\n",
      "ndcg    0.025000  0.053176  0.075817  0.035690\n",
      "recall  0.008333  0.080208  0.143750  0.041667\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631b91356d524d91b60b4aba36d6357a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 150: 'recall@10' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k              1        10        20         5\n",
      "map     0.034375  0.030553  0.036934  0.026788\n",
      "ndcg    0.034375  0.055916  0.078547  0.045150\n",
      "recall  0.011458  0.079167  0.142708  0.055208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from replay.nn.lightning.callbacks import ComputeMetricsCallback\n",
    "from replay.nn.lightning.postprocessors import SeenItemsFilter\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\".checkpoints\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"recall@10\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "postprocessors = [\n",
    "    SeenItemsFilter(\n",
    "        seen_path=VAL_PATH,\n",
    "        item_count=NUM_UNIQUE_ITEMS,\n",
    "        query_column=\"user_id\",\n",
    "        item_column=tensor_schema.item_id_feature_name,\n",
    "    )\n",
    "]\n",
    "validation_metrics_callback = ComputeMetricsCallback(\n",
    "    metrics=[\"map\", \"ndcg\", \"recall\"],\n",
    "    ks=[1, 5, 10, 20],\n",
    "    item_count=NUM_UNIQUE_ITEMS,\n",
    "    postprocessors=postprocessors\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/train\", name=\"SasRec-example\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint_callback, validation_metrics_callback],\n",
    "    logger=csv_logger,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=streaming_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now laod the best model using the path stored in the callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = LightningModule.load_from_checkpoint(checkpoint_callback.best_model_path, model=sasrec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference\n",
    "We can now perform inference using the data module we created earlier. Recommendations can be fetched in four formats: PySpark DataFrame, Pandas DataFrame, Polars DataFrame or raw PyTorch tensors. Each of the types corresponds a callback. Inthis example, we'll be using the `PandasTopItemsCallback`.\n",
    "Prediction callbacks also can filter results using postprocessors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6a5abe9b6649e384069edace8237d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from replay.nn.lightning.callbacks import PandasTopItemsCallback\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/test\", name=\"SasRec-example\")\n",
    "\n",
    "TOPK = [1, 5, 10, 20]\n",
    "\n",
    "postprocessors = [\n",
    "    SeenItemsFilter(\n",
    "        seen_path=TEST_PATH,\n",
    "        item_count=NUM_UNIQUE_ITEMS,\n",
    "        query_column=\"user_id\",\n",
    "        item_column=tensor_schema.item_id_feature_name,\n",
    "    )\n",
    "]\n",
    "\n",
    "pandas_prediction_callback = PandasTopItemsCallback(\n",
    "    top_k=max(TOPK),\n",
    "    query_column=\"user_id\",\n",
    "    item_column=\"item_id\",\n",
    "    rating_column=\"score\",\n",
    "    postprocessors=postprocessors,\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(callbacks=[pandas_prediction_callback], logger=csv_logger, inference_mode=True)\n",
    "\n",
    "trainer.predict(best_model, datamodule=streaming_dataset, return_predictions=False)\n",
    "\n",
    "pandas_res = pandas_prediction_callback.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1506</td>\n",
       "      <td>4.373491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1011</td>\n",
       "      <td>4.034044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>853</td>\n",
       "      <td>4.02631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1491</td>\n",
       "      <td>3.801472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1490</td>\n",
       "      <td>3.697806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>938</td>\n",
       "      <td>1052</td>\n",
       "      <td>2.429619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>938</td>\n",
       "      <td>1374</td>\n",
       "      <td>2.427046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>938</td>\n",
       "      <td>679</td>\n",
       "      <td>2.367162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>938</td>\n",
       "      <td>1649</td>\n",
       "      <td>2.346843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>938</td>\n",
       "      <td>789</td>\n",
       "      <td>2.344668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18860 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id item_id     score\n",
       "0          3    1506  4.373491\n",
       "0          3    1011  4.034044\n",
       "0          3     853   4.02631\n",
       "0          3    1491  3.801472\n",
       "0          3    1490  3.697806\n",
       "..       ...     ...       ...\n",
       "942      938    1052  2.429619\n",
       "942      938    1374  2.427046\n",
       "942      938     679  2.367162\n",
       "942      938    1649  2.346843\n",
       "942      938     789  2.344668\n",
       "\n",
       "[18860 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_metrics = OfflineMetrics(\n",
    "    [Recall(TOPK), Precision(TOPK), MAP(TOPK)], query_column=\"user_id\", rating_column=\"score\"\n",
    ")(pandas_res, test_gt.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>k</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAP</th>\n",
       "      <td>0.053022</td>\n",
       "      <td>0.045232</td>\n",
       "      <td>0.053110</td>\n",
       "      <td>0.037127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.053022</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>0.028897</td>\n",
       "      <td>0.041145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.017674</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.192648</td>\n",
       "      <td>0.068575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "k                 1        10        20         5\n",
       "MAP        0.053022  0.045232  0.053110  0.037127\n",
       "Precision  0.053022  0.034783  0.028897  0.041145\n",
       "Recall     0.017674  0.115942  0.192648  0.068575"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_to_df(result_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "replay_tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
