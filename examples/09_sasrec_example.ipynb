{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of SasRec training/inference with Parquet Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import lightning as L\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import torch\n",
    "\n",
    "from replay.data import (\n",
    "    FeatureHint,\n",
    "    FeatureSource,\n",
    "    FeatureType,\n",
    ")\n",
    "from replay.data.nn import (\n",
    "    TensorFeatureInfo,\n",
    "    TensorFeatureSource,\n",
    "    TensorSchema,\n",
    ")\n",
    "from replay.metrics import MAP, OfflineMetrics, Precision, Recall\n",
    "from replay.metrics.torch_metrics_builder import metrics_to_df\n",
    "\n",
    "L.seed_everything(42)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data\n",
    "In this example, we will be using the MovieLens dataset, namely the 1m subset. It's demonstrated a simple case, so only item ids will be used as model input.\n",
    "\n",
    "---\n",
    "**NOTE**\n",
    "\n",
    "Current implementation of SasRec is able to handle item and interactions features. It does not take into account user features. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_csv(\"./data/ml1m_ratings.dat\", sep=\"\\t\", names=[\"user_id\", \"item_id\",\"rating\",\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000138</th>\n",
       "      <td>6040</td>\n",
       "      <td>858</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000153</th>\n",
       "      <td>6040</td>\n",
       "      <td>2384</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999873</th>\n",
       "      <td>6040</td>\n",
       "      <td>593</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000192</th>\n",
       "      <td>6040</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000007</th>\n",
       "      <td>6040</td>\n",
       "      <td>1961</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825793</th>\n",
       "      <td>4958</td>\n",
       "      <td>2399</td>\n",
       "      <td>1</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825438</th>\n",
       "      <td>4958</td>\n",
       "      <td>1407</td>\n",
       "      <td>5</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825731</th>\n",
       "      <td>4958</td>\n",
       "      <td>2634</td>\n",
       "      <td>3</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825724</th>\n",
       "      <td>4958</td>\n",
       "      <td>3264</td>\n",
       "      <td>4</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825603</th>\n",
       "      <td>4958</td>\n",
       "      <td>1924</td>\n",
       "      <td>4</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000209 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id  rating  timestamp\n",
       "1000138     6040      858       4          0\n",
       "1000153     6040     2384       4          1\n",
       "999873      6040      593       5          2\n",
       "1000192     6040     2019       5          3\n",
       "1000007     6040     1961       4          4\n",
       "...          ...      ...     ...        ...\n",
       "825793      4958     2399       1        446\n",
       "825438      4958     1407       5        447\n",
       "825731      4958     2634       3        448\n",
       "825724      4958     3264       4        449\n",
       "825603      4958     1924       4        450\n",
       "\n",
       "[1000209 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions[\"timestamp\"] = interactions[\"timestamp\"].astype(\"int64\")\n",
    "interactions = interactions.sort_values(by=\"timestamp\")\n",
    "interactions[\"timestamp\"] = interactions.groupby(\"user_id\").cumcount()\n",
    "interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode catagorical data.\n",
    "To ensure all categorical data is fit for training, it needs to be encoded using the `LabelEncoder` class. Create an instance of the encoder, providing a `LabelEncodingRule` for each categorcial column in the dataset that will be used in model. Note that ids of users and ids of items are always used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>339</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000204</th>\n",
       "      <td>4</td>\n",
       "      <td>281</td>\n",
       "      <td>796</td>\n",
       "      <td>3705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000205</th>\n",
       "      <td>4</td>\n",
       "      <td>209</td>\n",
       "      <td>1297</td>\n",
       "      <td>3705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000206</th>\n",
       "      <td>4</td>\n",
       "      <td>748</td>\n",
       "      <td>1883</td>\n",
       "      <td>3705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000207</th>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>4449</td>\n",
       "      <td>3705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000208</th>\n",
       "      <td>4</td>\n",
       "      <td>287</td>\n",
       "      <td>2473</td>\n",
       "      <td>3705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000209 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rating  timestamp  user_id  item_id\n",
       "0             4         32        0        0\n",
       "1             4         10        1        0\n",
       "2             5         12        2        0\n",
       "3             4        339        3        0\n",
       "4             4        144        4        0\n",
       "...         ...        ...      ...      ...\n",
       "1000204       4        281      796     3705\n",
       "1000205       4        209     1297     3705\n",
       "1000206       4        748     1883     3705\n",
       "1000207       5         71     4449     3705\n",
       "1000208       4        287     2473     3705\n",
       "\n",
       "[1000209 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from replay.preprocessing.label_encoder import LabelEncoder, LabelEncodingRule\n",
    "\n",
    "encoder = LabelEncoder(\n",
    "    [\n",
    "        LabelEncodingRule(\"user_id\", default_value=\"last\"),\n",
    "        LabelEncodingRule(\"item_id\", default_value=\"last\"),\n",
    "    ]\n",
    ")\n",
    "interactions = interactions.sort_values(by=\"item_id\", ascending=True)\n",
    "encoded_interactions = encoder.fit_transform(interactions)\n",
    "encoded_interactions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split interactions into the train, validation and test datasets using LastNSplitter\n",
    "We use widespread splitting strategy Last-One-Out. We filter out cold items and users for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.splitters import LastNSplitter\n",
    "\n",
    "splitter = LastNSplitter(\n",
    "    N=1,\n",
    "    divide_column=\"user_id\",\n",
    "    query_column=\"user_id\",\n",
    "    strategy=\"interactions\",\n",
    "    drop_cold_users=True,\n",
    "    drop_cold_items=True\n",
    ")\n",
    "\n",
    "test_events, test_gt = splitter.split(encoded_interactions)\n",
    "validation_events, validation_gt = splitter.split(test_events)\n",
    "train_events = validation_events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing (\"baking\")\n",
    "SasRec expects each user in the batch to provide their events in form of a sequence. For this reason, the event splits must be properly processed using the `groupby_sequences` function provided by RePlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[5, 5, 1, 3, 5, 5, 5, 5, 5, 5, 1, 1, 1, 5, 5, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[859, 309, 2371, 3442, 1108, 329, 367, 3279, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[3, 3, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[354, 2426, 253, 1371, 513, 1184, 3131, 309, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[4, 3, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[1008, 1120, 2439, 1066, 3197, 253, 1108, 1107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[4, 2, 4, 4, 5, 2, 3, 5, 3, 4, 5, 3, 4, 5, 3, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[346, 2190, 670, 802, 323, 661, 2480, 2501, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[1120, 758, 2426, 1838, 2621, 3341, 3377, 3502...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6035</th>\n",
       "      <td>6035</td>\n",
       "      <td>[5, 4, 4, 4, 4, 5, 4, 5, 5, 3, 1, 4, 4, 4, 3, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[2426, 1279, 3321, 3151, 1178, 2501, 3301, 248...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6036</th>\n",
       "      <td>6036</td>\n",
       "      <td>[3, 3, 5, 2, 4, 5, 5, 4, 4, 3, 5, 4, 5, 5, 5, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[1592, 2302, 1633, 1813, 2879, 1482, 2651, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6037</td>\n",
       "      <td>[4, 5, 1, 2, 3, 5, 3, 5, 4, 5, 5, 4, 3, 4, 3, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[1971, 3500, 1666, 2077, 1399, 2748, 2958, 278...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>6038</td>\n",
       "      <td>[5, 5, 5, 3, 2, 4, 5, 4, 4, 5, 5, 5, 4, 4, 5, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[1486, 1485, 3384, 3512, 3302, 3126, 3650, 330...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6039</th>\n",
       "      <td>6039</td>\n",
       "      <td>[2, 1, 2, 4, 1, 4, 4, 5, 5, 1, 2, 4, 5, 4, 3, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[2432, 2960, 2114, 1848, 2142, 3248, 3091, 317...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6040 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                                             rating  \\\n",
       "0           0  [5, 5, 1, 3, 5, 5, 5, 5, 5, 5, 1, 1, 1, 5, 5, ...   \n",
       "1           1  [3, 3, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 5, ...   \n",
       "2           2  [4, 3, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...   \n",
       "3           3  [4, 2, 4, 4, 5, 2, 3, 5, 3, 4, 5, 3, 4, 5, 3, ...   \n",
       "4           4  [5, 4, 4, 5, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4, ...   \n",
       "...       ...                                                ...   \n",
       "6035     6035  [5, 4, 4, 4, 4, 5, 4, 5, 5, 3, 1, 4, 4, 4, 3, ...   \n",
       "6036     6036  [3, 3, 5, 2, 4, 5, 5, 4, 4, 3, 5, 4, 5, 5, 5, ...   \n",
       "6037     6037  [4, 5, 1, 2, 3, 5, 3, 5, 4, 5, 5, 4, 3, 4, 3, ...   \n",
       "6038     6038  [5, 5, 5, 3, 2, 4, 5, 4, 4, 5, 5, 5, 4, 4, 5, ...   \n",
       "6039     6039  [2, 1, 2, 4, 1, 4, 4, 5, 5, 1, 2, 4, 5, 4, 3, ...   \n",
       "\n",
       "                                              timestamp  \\\n",
       "0     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "1     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "2     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "3     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "4     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "...                                                 ...   \n",
       "6035  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "6036  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "6037  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "6038  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "6039  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "\n",
       "                                                item_id  \n",
       "0     [859, 309, 2371, 3442, 1108, 329, 367, 3279, 7...  \n",
       "1     [354, 2426, 253, 1371, 513, 1184, 3131, 309, 8...  \n",
       "2     [1008, 1120, 2439, 1066, 3197, 253, 1108, 1107...  \n",
       "3     [346, 2190, 670, 802, 323, 661, 2480, 2501, 19...  \n",
       "4     [1120, 758, 2426, 1838, 2621, 3341, 3377, 3502...  \n",
       "...                                                 ...  \n",
       "6035  [2426, 1279, 3321, 3151, 1178, 2501, 3301, 248...  \n",
       "6036  [1592, 2302, 1633, 1813, 2879, 1482, 2651, 200...  \n",
       "6037  [1971, 3500, 1666, 2077, 1399, 2748, 2958, 278...  \n",
       "6038  [1486, 1485, 3384, 3512, 3302, 3126, 3650, 330...  \n",
       "6039  [2432, 2960, 2114, 1848, 2142, 3248, 3091, 317...  \n",
       "\n",
       "[6040 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from replay.data.nn.utils import groupby_sequences\n",
    "\n",
    "\n",
    "def bake_data(full_data):\n",
    "    grouped_interactions = groupby_sequences(events=full_data, groupby_col=\"user_id\", sort_col=\"timestamp\")\n",
    "    return grouped_interactions\n",
    "\n",
    "\n",
    "train_events = bake_data(train_events)\n",
    "\n",
    "validation_events = bake_data(validation_events)\n",
    "validation_gt = bake_data(validation_gt)\n",
    "\n",
    "test_events = bake_data(test_events)\n",
    "test_gt = bake_data(test_gt)\n",
    "\n",
    "train_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we don't have unknown users in ground truth, we join validation events and validation ground truth (also join test events and test ground truth correspondingly) by user ids to leave only the common ones.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gt_to_events(events_df, gt_df):\n",
    "    # Keep common user ids between events_df and gt_df\n",
    "    events_df = events_df[events_df[\"user_id\"].isin(gt_df[\"user_id\"])]\n",
    "    gt_df = gt_df[gt_df[\"user_id\"].isin(events_df[\"user_id\"])]\n",
    "\n",
    "    gt_to_join = gt_df.loc[:, [\"user_id\", \"item_id\"]].rename(columns={\"item_id\": \"ground_truth\"})\n",
    "\n",
    "    events_df = events_df.merge(gt_to_join, on=\"user_id\", how=\"left\")\n",
    "    return events_df\n",
    "\n",
    "validation_events = add_gt_to_events(validation_events, validation_gt)\n",
    "test_events = add_gt_to_events(test_events, test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"temp/data/\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_PATH = data_dir / \"train.parquet\"\n",
    "VAL_PATH = data_dir / \"val.parquet\"\n",
    "TEST_PATH = data_dir / \"test.parquet\"\n",
    "\n",
    "ENCODER_PATH = data_dir / \"encoder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use the ParquetModule for batch-wise data reading. It is based on PyArrow which requires explicit matching types, so to save via pandas you need to add a PyArrow data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_schema = pa.schema([\n",
    "            (\"item_id\", pa.list_(pa.int64())),\n",
    "            (\"user_id\", pa.int64()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_test_schema = pa.schema([\n",
    "            (\"item_id\", pa.list_(pa.int64())),\n",
    "            (\"ground_truth\", pa.list_(pa.int64())),\n",
    "            (\"user_id\", pa.int64()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "train_events.loc[:, [\"user_id\", \"item_id\"]].to_parquet(TRAIN_PATH, schema=train_schema)\n",
    "validation_events.loc[:, [\"user_id\", \"item_id\", \"ground_truth\"]].to_parquet(VAL_PATH, schema=val_test_schema)\n",
    "test_events.loc[:, [\"user_id\", \"item_id\", \"ground_truth\"]].to_parquet(TEST_PATH, schema=val_test_schema)\n",
    "\n",
    "encoder.save(ENCODER_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare to model training\n",
    "### Create the tensor schema\n",
    "A schema shows the correspondence of columns from the source dataset with the internal representation of tensors inside the model. It is required by the SasRec model to correctly create embeddings for every source column. Note that user_id does not required in `TensorSchema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "\n",
    "ITEM_FEATURE_NAME = \"item_id\"\n",
    "\n",
    "encoder = encoder.load(ENCODER_PATH)\n",
    "NUM_UNIQUE_ITEMS = len(encoder.mapping[\"item_id\"])\n",
    "\n",
    "tensor_schema = TensorSchema(\n",
    "    [\n",
    "        TensorFeatureInfo(\n",
    "            name=\"item_id\",\n",
    "            is_seq=True,\n",
    "            padding_value=NUM_UNIQUE_ITEMS,\n",
    "            cardinality=NUM_UNIQUE_ITEMS + 1,  # taking into account padding\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            feature_type=FeatureType.CATEGORICAL,\n",
    "            feature_sources=[TensorFeatureSource(FeatureSource.INTERACTIONS, \"item_id\")],\n",
    "            feature_hint=FeatureHint.ITEM_ID,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure ParquetModule and transformation pipelines\n",
    "\n",
    "The `ParquetModule` class enables training of models on large datasets by reading data in batch-wise way. This class initialized with  paths to every data split, a metadata dict containing information about shape and padding value of every column and a dict of transforms. `ParquetModule`'s  \"transform pipelines\" are stage-specific modules implementing additional preprocessing to be performed on batch level right before the forward pass.  \n",
    "\n",
    "For SasRec model, RePlay provides a function that generates a sequence of appropriate transforms for each data split.\n",
    "\n",
    "Internally this function creates the following transforms:\n",
    "1) Training:\n",
    "    1. Create a target, which contains the shifted item sequence that represents the next item in the sequence (for the next item prediction task).\n",
    "    2. Rename features to match it with expected format by the model during training.\n",
    "    3. Unsqueeze target (*positive_labels*) and it's padding mask (*target_padding_mask*) for getting required shape of this tensors for loss computation.\n",
    "    4. Group input features to be embed in expected format.\n",
    "\n",
    "2) Validation/Inference:\n",
    "    1. Rename/group features to match it with expected format by the model during valdiation/inference.\n",
    "\n",
    "**Note:** One of the transforms for the training data prepares the initial sequence for the task of Next Item Prediction so it shifts the sequence of items. For the final sequence length to be correct, you need to set shape of item_id in metadata as **model sequence length + shift**. Default shift value is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.nn.transforms.templates.sasrec import make_default_sasrec_transforms\n",
    "\n",
    "MAX_SEQ_LEN = 50\n",
    "BATCH_SIZE = 32\n",
    "SHIFT = 1\n",
    "\n",
    "TRANSFORMS = make_default_sasrec_transforms(tensor_schema, query_column=\"user_id\")\n",
    "\n",
    "def create_meta(shape: int, gt_shape: Optional[int] = None):\n",
    "    meta = {\n",
    "        \"user_id\": {},\n",
    "        \"item_id\": {\"shape\": shape, \"padding\": tensor_schema[\"item_id\"].padding_value},\n",
    "    }\n",
    "    if gt_shape is not None:\n",
    "        meta.update({\"ground_truth\": {\"shape\": gt_shape, \"padding\": -1}})\n",
    "\n",
    "    return meta\n",
    "\n",
    "METADATA = {\n",
    "    \"train\": create_meta(shape=MAX_SEQ_LEN+1),\n",
    "    \"validate\": create_meta(shape=MAX_SEQ_LEN, gt_shape=1),\n",
    "    \"test\": create_meta(shape=MAX_SEQ_LEN, gt_shape=1),\n",
    "    \"predict\": create_meta(shape=MAX_SEQ_LEN)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.data.nn import ParquetModule\n",
    "\n",
    "parquet_module = ParquetModule(\n",
    "    train_path=TRAIN_PATH,\n",
    "    validate_path=VAL_PATH,\n",
    "    test_path=TEST_PATH,\n",
    "    predict_path=TEST_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    metadata=METADATA,\n",
    "    transforms=TRANSFORMS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: \n",
    "You can also create a module specifically for training/inference by providing only their respective datapaths.\n",
    "In such cases it's possible to pass to ParquetModule either all transforms or transforms for used data splits only.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_module_train_val = ParquetModule(\n",
    "    train_path=TRAIN_PATH,\n",
    "    validate_path=VAL_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    metadata=METADATA,\n",
    "    transforms=TRANSFORMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "### Create SasRec model instance and run the training stage using lightning\n",
    "We may now train the model using the Lightning trainer class. \n",
    "\n",
    "RePlay's implementation of SasRec is designed in a modular, **block-based approach**. Instead of passing configuration parameters to the constructor, SasRec is now built by providing fully initialized components that makes the model more flexible and easier to extend. SasRec consists of the body and loss. Body consits of the following components: embedder, aggregator, encoder, mask, output_normalization, loss.\n",
    "\n",
    "#### Components of SasRec\n",
    "* `Body` - The body component defines the full model excluding loss.\n",
    "* `Loss` - The loss component defines how the training loss is computed. All available loss implementations are located in nn/loss.\n",
    "\n",
    "#### Components of SasRecBody\n",
    "\n",
    "* `Embedder` -The embedder is responsible for converting input features into embeddings. The default implementation is `SequenceEmbedding`, which supports the following feature types: categorical, categorical_list, numerical, numerical_list\n",
    "\n",
    "* `Aggregator` - The aggregator combines all embeddings produced by the embedder and adds positional embeddings.\n",
    "Currently, `SasRecAggregator` is supported. It internally uses one of the following embedding aggregation strategies: `SumAggregator`, `ConcatAggregator`.\n",
    "\n",
    "* `Encoder` - The encoder represents the core transformer block of the model. The following implementations are currently available: `SasRecTransformerLayer` (default one), `DiffAttentionLayer` (a modified version with differential attention).\n",
    "\n",
    "* `Mask` - The mask is an object that creates attention mask by input. RePlay supports `DefaultAttentionMask` creating a lower-triangular attention mask.\n",
    "\n",
    "* `Output Normalization` - Any suitable PyTorch normalization layer may be used as output_normalization, for example: torch.nn.LayerNorm or torch.nn.RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.nn import DefaultAttentionMask, SequenceEmbedding, SumAggregator\n",
    "from replay.nn.loss import CE\n",
    "from replay.nn.sequential import PositionAwareAggregator, SasRec, SasRecBody, SasRecTransformerLayer\n",
    "\n",
    "\n",
    "def create_sasrec_model(tensor_schema: TensorSchema,\n",
    "                        embedding_dim: int = 256,\n",
    "                        categorical_list_feature_aggregation_method: str = \"sum\",\n",
    "                        max_seq_len: int = 50,\n",
    "                        dropout: float = 0.2,\n",
    "                        num_heads: int = 2,\n",
    "                        num_blocks: int = 2,\n",
    "                        activation=\"relu\"\n",
    "    ):\n",
    "    body = SasRecBody(\n",
    "        embedder=SequenceEmbedding(\n",
    "            schema=tensor_schema,\n",
    "            categorical_list_feature_aggregation_method=categorical_list_feature_aggregation_method,\n",
    "        ),\n",
    "        embedding_aggregator=PositionAwareAggregator(\n",
    "            embedding_aggregator=SumAggregator(embedding_dim=embedding_dim),\n",
    "            max_sequence_length=max_seq_len,\n",
    "            dropout=dropout,\n",
    "        ),\n",
    "        attn_mask_builder=DefaultAttentionMask(\n",
    "            reference_feature_name=tensor_schema.item_id_feature_name,\n",
    "            num_heads=num_heads,\n",
    "        ),\n",
    "        encoder=SasRecTransformerLayer(\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_blocks=num_blocks,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "        ),\n",
    "        output_normalization=torch.nn.LayerNorm(embedding_dim),\n",
    "    )\n",
    "    sasrec = SasRec(\n",
    "        body=body,\n",
    "        loss=CE(padding_idx=tensor_schema.item_id_features.item().padding_value),\n",
    "    )\n",
    "    return sasrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BLOCKS = 2\n",
    "NUM_HEADS = 2\n",
    "DROPOUT = 0.3\n",
    "sasrec = create_sasrec_model(\n",
    "    tensor_schema,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    dropout=DROPOUT,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_blocks=NUM_BLOCKS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Configuration\n",
    "\n",
    "Default SasRec model may be created quickly via method *from_params*. Default model instance has CE loss, original SasRec transformer layes, and embeddings are aggregated via sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_sasrec = SasRec.from_params(\n",
    "    schema=tensor_schema,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_sequence_length=MAX_SEQ_LEN,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    dropout=DROPOUT,\n",
    "    excluded_features=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A universal PyTorch Lightning module is provided that can work with any RePlay NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay.models.nn.optimizer_utils import FatLRSchedulerFactory, FatOptimizerFactory\n",
    "from replay.nn.lightning import LightningModule\n",
    "\n",
    "model = LightningModule(\n",
    "    sasrec,\n",
    "    optimizer_factory=FatOptimizerFactory(),\n",
    "    lr_scheduler_factory=FatLRSchedulerFactory(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate training, we add the following callbacks:\n",
    "1) `ModelCheckpoint` - to save the best trained model based on its Recall metric. It's a default Lightning Callback.\n",
    "1) `ComputeMetricsCallback` - to display a detailed validation metric matrix after each epoch. It's a custom RePlay callback for computing recsys metrics on validation and test stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type   | Params | Mode \n",
      "-----------------------------------------\n",
      "0 | model | SasRec | 291 K  | train\n",
      "-----------------------------------------\n",
      "291 K     Trainable params\n",
      "0         Non-trainable params\n",
      "291 K     Total params\n",
      "1.164     Total estimated model params size (MB)\n",
      "39        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6ed982d48c481faad3c52e4e3fc034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce262be0de249acbf5f87403fb21c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffc7f95a67b4d65b5a1f90de0bd5b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 189: 'recall@10' reached 0.03709 (best 0.03709), saving model to '/home/replay/sasrec/checkpoints/epoch=0-step=189.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k             1         5         10        20\n",
      "map     0.002649  0.007919  0.010259  0.012502\n",
      "ndcg    0.002649  0.010639  0.016405  0.024654\n",
      "recall  0.002649  0.019040  0.037086  0.069868\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acac13a282f949b88c07c567f85813fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 378: 'recall@10' reached 0.09023 (best 0.09023), saving model to '/home/replay/sasrec/checkpoints/epoch=1-step=378.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k             1         5         10        20\n",
      "map     0.011424  0.024255  0.029552  0.033440\n",
      "ndcg    0.011424  0.030393  0.043487  0.057861\n",
      "recall  0.011424  0.049172  0.090232  0.147517\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deaf9b23043a4ea480e748889b9c0021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 567: 'recall@10' reached 0.12649 (best 0.12649), saving model to '/home/replay/sasrec/checkpoints/epoch=2-step=567.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k             1         5         10        20\n",
      "map     0.015728  0.035381  0.042149  0.047407\n",
      "ndcg    0.015728  0.045037  0.061643  0.081116\n",
      "recall  0.015728  0.074669  0.126490  0.204139\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f50c3c7094d4c1ba4bdcd55c267bd81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 756: 'recall@10' reached 0.14040 (best 0.14040), saving model to '/home/replay/sasrec/checkpoints/epoch=3-step=756.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k             1         5         10        20\n",
      "map     0.013576  0.035381  0.043214  0.048966\n",
      "ndcg    0.013576  0.046365  0.065645  0.086942\n",
      "recall  0.013576  0.080132  0.140397  0.225331\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a08f1ca0464067a07784aff9a3366b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 945: 'recall@10' reached 0.15315 (best 0.15315), saving model to '/home/replay/sasrec/checkpoints/epoch=4-step=945.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k             1         5         10        20\n",
      "map     0.017219  0.040811  0.048909  0.055372\n",
      "ndcg    0.017219  0.053191  0.073015  0.096932\n",
      "recall  0.017219  0.091391  0.153146  0.248510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from replay.nn.lightning.callbacks import ComputeMetricsCallback\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"sasrec/checkpoints\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"recall@10\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "validation_metrics_callback = ComputeMetricsCallback(\n",
    "    metrics=[\"map\", \"ndcg\", \"recall\"],\n",
    "    ks=[1, 5, 10, 20],\n",
    "    item_count=NUM_UNIQUE_ITEMS,\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\"sasrec/logs/train\", name=\"SasRec-example\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=[checkpoint_callback, validation_metrics_callback],\n",
    "    logger=csv_logger,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=parquet_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the best model path stored in the checkpoint callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/replay/sasrec/checkpoints/epoch=4-step=945.ckpt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path = checkpoint_callback.best_model_path\n",
    "best_model_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference&Test stages\n",
    "\n",
    "To obtain model scores, we will load the weights from the best checkpoint. To do this, we use the LightningModule, provide there the path to the checkpoint and the model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sasrec = create_sasrec_model(\n",
    "    tensor_schema,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    dropout=DROPOUT,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_blocks=NUM_BLOCKS\n",
    "    )\n",
    "\n",
    "best_model = LightningModule.load_from_checkpoint(best_model_path, model=sasrec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluding train and validation, PyTorch Lightning supports 2 different stages: **test** and **predict**. \n",
    "\n",
    "* Test stage is used just for metric calculation, so it requires ground truth.\n",
    "\n",
    "* Predict stage is used for obtaining inference scores. In our case, we are calculating metrics so we can calculate them manually by inference scores.\n",
    "\n",
    "### Test stage\n",
    "Here we can use the same callback as in validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a73120c5fab4e819da84ec7965f08f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k             1         5         10        20\n",
      "map     0.017224  0.040027  0.047906  0.053678\n",
      "ndcg    0.017224  0.051358  0.070750  0.091997\n",
      "recall  0.017224  0.086121  0.146737  0.231202\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "          map@1            0.017224246636033058\n",
      "         map@10            0.047905657440423965\n",
      "         map@20             0.05367831140756607\n",
      "          map@5             0.04002705216407776\n",
      "         ndcg@1            0.017224246636033058\n",
      "         ndcg@10            0.0707496926188469\n",
      "         ndcg@20            0.09199724346399307\n",
      "         ndcg@5             0.05135849490761757\n",
      "        recall@1           0.017224246636033058\n",
      "        recall@10           0.14673733711242676\n",
      "        recall@20           0.2312023788690567\n",
      "        recall@5            0.08612123131752014\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "test_metrics_callback = ComputeMetricsCallback(\n",
    "    metrics=[\"map\", \"ndcg\", \"recall\"],\n",
    "    ks=[1, 5, 10, 20],\n",
    "    item_count=NUM_UNIQUE_ITEMS,\n",
    ")\n",
    "trainer = L.Trainer(callbacks=[test_metrics_callback], inference_mode=True)\n",
    "trainer.test(best_model, datamodule=parquet_module);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference stage\n",
    "\n",
    "During inference, we can use another type of callback from the RePlay's *TopItemsCallback. Such callbacks allow you to get scores for each user throughout the entire catalog and get recommendations in the form of ids of items with the highest score values.\n",
    "\n",
    "\n",
    "Recommendations can be fetched in four formats: PySpark DataFrame, Pandas DataFrame, Polars DataFrame or raw PyTorch tensors. Each of the types corresponds a callback. In this example, we'll be using the `PandasTopItemsCallback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34813279ad9645d0a98f604b48b1731a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from replay.nn.lightning.callbacks import PandasTopItemsCallback\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/test\", name=\"SasRec-example\")\n",
    "\n",
    "TOPK = [1, 5, 10, 20]\n",
    "\n",
    "pandas_prediction_callback = PandasTopItemsCallback(\n",
    "    top_k=max(TOPK),\n",
    "    query_column=\"user_id\",\n",
    "    item_column=\"item_id\",\n",
    "    rating_column=\"score\",\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(callbacks=[pandas_prediction_callback], logger=csv_logger, inference_mode=True)\n",
    "\n",
    "trainer.predict(best_model, datamodule=parquet_module, return_predictions=False)\n",
    "\n",
    "pandas_res = pandas_prediction_callback.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3341</td>\n",
       "      <td>6.163676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3383</td>\n",
       "      <td>6.109334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3510</td>\n",
       "      <td>5.963237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3550</td>\n",
       "      <td>5.852643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3512</td>\n",
       "      <td>5.571964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6039</td>\n",
       "      <td>2601</td>\n",
       "      <td>5.27216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6039</td>\n",
       "      <td>2470</td>\n",
       "      <td>5.235597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6039</td>\n",
       "      <td>2750</td>\n",
       "      <td>5.109411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6039</td>\n",
       "      <td>2700</td>\n",
       "      <td>5.101314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6039</td>\n",
       "      <td>3555</td>\n",
       "      <td>5.03313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120760 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id item_id     score\n",
       "0           0    3341  6.163676\n",
       "0           0    3383  6.109334\n",
       "0           0    3510  5.963237\n",
       "0           0    3550  5.852643\n",
       "0           0    3512  5.571964\n",
       "...       ...     ...       ...\n",
       "6037     6039    2601   5.27216\n",
       "6037     6039    2470  5.235597\n",
       "6037     6039    2750  5.109411\n",
       "6037     6039    2700  5.101314\n",
       "6037     6039    3555   5.03313\n",
       "\n",
       "[120760 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating metrics\n",
    "\n",
    "*test_gt* is already encoded, so we can use it for computing metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_metrics = OfflineMetrics(\n",
    "    [Recall(TOPK), Precision(TOPK), MAP(TOPK)], query_column=\"user_id\", rating_column=\"score\"\n",
    ")(pandas_res, test_gt.explode(\"item_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>k</th>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAP</th>\n",
       "      <td>0.017224</td>\n",
       "      <td>0.040027</td>\n",
       "      <td>0.047906</td>\n",
       "      <td>0.053678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.017224</td>\n",
       "      <td>0.017224</td>\n",
       "      <td>0.014674</td>\n",
       "      <td>0.011560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.017224</td>\n",
       "      <td>0.086121</td>\n",
       "      <td>0.146737</td>\n",
       "      <td>0.231202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "k                1         5         10        20\n",
       "MAP        0.017224  0.040027  0.047906  0.053678\n",
       "Precision  0.017224  0.017224  0.014674  0.011560\n",
       "Recall     0.017224  0.086121  0.146737  0.231202"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_to_df(result_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "replay-sasrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
