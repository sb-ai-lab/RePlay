{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RePlay working with multi-frameworks models\n",
    "\n",
    "We will show how you can work with models on different frameworks and compare performance of this new feature on well-known MovieLens dataset. If you have not used RePlay before, start with 01_replay_basics.ipynb which introduces base concepts and describe main classes and functionality.\n",
    "\n",
    "### Dataset\n",
    "We will compare RePlay models on __MovieLens 1m__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TERMS:\n",
    "- Implementation: A real model, that realize a common functions of Recommendation model: fit, predict, fit_predict...  \n",
    "This model is writen on one of frameworks - ``Pandas``, ``Polars`` or ``Spark``\n",
    "- Client: an object that contain link to 3 implementations - on Polars, on Pandas, on Spark.  \n",
    "It also provides convertation from one framework to other (for example - from spark-fitted model to pandas)  \n",
    "It wraps functions of implementation - usually, when you call ``BaseRecommenderClient.fit()``, inside this fit you call ``Client._impl.fit()`` add some functionality, for example - type-checking  \n",
    "- Convertation: Change the ``_impl`` link from one implementation to other. When you call ``to_pandas()``, it creates implementation of Pandas type, and Client change its ``_impl`` link to this new implementation  \n",
    "Also, we converts all DataFrameLike properties to selected framework type  \n",
    "All other properties are retain their values too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T16:01:45.639135Z",
     "start_time": "2020-02-10T16:01:45.612577Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from optuna.exceptions import ExperimentalWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ExperimentalWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "from pyspark.sql import functions as sf\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from replay.data import Dataset, FeatureHint, FeatureInfo, FeatureSchema, FeatureType\n",
    "from replay.data.dataset_utils import DatasetLabelEncoder\n",
    "from replay.models import PopRec\n",
    "from replay.models.implementations import _PopRecSpark, _PopRecPolars, _PopRecPandas\n",
    "\n",
    "from replay.utils.session_handler import State, get_spark_session\n",
    "from replay.utils.spark_utils import convert2spark, get_log_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`State` object allows passing existing Spark session or create a new one, which will be used by the all RePlay modules.\n",
    "\n",
    "To create session with custom parameters ``spark.driver.memory`` and ``spark.sql.shuffle.partitions`` use function `get_spark_session` from `session_handler` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T15:59:09.227179Z",
     "start_time": "2020-02-10T15:59:06.427348Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/31 05:54:38 WARN Utils: Your hostname, ecs-alaleksepetrov-2 resolves to a loopback address: 127.0.1.1; using 10.11.12.199 instead (on interface eth0)\n",
      "25/03/31 05:54:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/31 05:54:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/31 05:54:39 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "25/03/31 05:54:40 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/03/31 05:54:40 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "25/03/31 05:54:40 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f17dd2e94d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = State().session\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"replay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "BUDGET = 5\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preprocessing <a name='data-preparator'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T15:59:42.041251Z",
     "start_time": "2020-02-10T15:59:09.230636Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000209 entries, 0 to 1000208\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count    Dtype\n",
      "---  ------     --------------    -----\n",
      " 0   user_id    1000209 non-null  int64\n",
      " 1   item_id    1000209 non-null  int64\n",
      " 2   rating     1000209 non-null  int64\n",
      " 3   timestamp  1000209 non-null  int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 30.5 MB\n",
      "None\n",
      "users\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6040 entries, 0 to 6039\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   user_id     6040 non-null   int64 \n",
      " 1   gender      6040 non-null   object\n",
      " 2   age         6040 non-null   int64 \n",
      " 3   occupation  6040 non-null   int64 \n",
      " 4   zip_code    6040 non-null   object\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 236.1+ KB\n",
      "None\n",
      "items\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3883 entries, 0 to 3882\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   item_id   3883 non-null   int64 \n",
      " 1   name      3883 non-null   object\n",
      " 2   category  3883 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 91.1+ KB\n",
      "None\n",
      "      item_id                                name  \\\n",
      "0           1                    Toy Story (1995)   \n",
      "1           2                      Jumanji (1995)   \n",
      "2           3             Grumpier Old Men (1995)   \n",
      "3           4            Waiting to Exhale (1995)   \n",
      "4           5  Father of the Bride Part II (1995)   \n",
      "...       ...                                 ...   \n",
      "3878     3948             Meet the Parents (2000)   \n",
      "3879     3949          Requiem for a Dream (2000)   \n",
      "3880     3950                    Tigerland (2000)   \n",
      "3881     3951             Two Family House (2000)   \n",
      "3882     3952               Contender, The (2000)   \n",
      "\n",
      "                          category  \n",
      "0      Animation|Children's|Comedy  \n",
      "1     Adventure|Children's|Fantasy  \n",
      "2                   Comedy|Romance  \n",
      "3                     Comedy|Drama  \n",
      "4                           Comedy  \n",
      "...                            ...  \n",
      "3878                        Comedy  \n",
      "3879                         Drama  \n",
      "3880                         Drama  \n",
      "3881                         Drama  \n",
      "3882                Drama|Thriller  \n",
      "\n",
      "[3883 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import replay\n",
    "from os.path import dirname, join\n",
    "\n",
    "dir = dirname(replay.__file__)\n",
    "pandas_ratings = pd.read_csv(join(dir, \"../examples/data/ml1m_ratings.dat\"), sep=\"\\t\", names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "pandas_users = pd.read_csv(join(dir, \"../examples/data/ml1m_users.dat\"), sep=\"\\t\", names=[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip_code\"])\n",
    "pandas_items =  pd.read_csv(join(dir, \"../examples/data/ml1m_items.dat\"), sep=\"\\t\", names=[\"item_id\", \"name\", \"category\"])\n",
    "print(\"ratings\")\n",
    "print(pandas_ratings.info())\n",
    "print(\"users\")\n",
    "print(pandas_users.info())\n",
    "print(\"items\")\n",
    "print(pandas_items.info())\n",
    "print(pandas_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_schema = FeatureSchema(\n",
    "    [\n",
    "        FeatureInfo(\n",
    "            column=\"user_id\",\n",
    "            feature_type=FeatureType.CATEGORICAL,\n",
    "            feature_hint=FeatureHint.QUERY_ID,\n",
    "        ),\n",
    "        FeatureInfo(\n",
    "            column=\"item_id\",\n",
    "            feature_type=FeatureType.CATEGORICAL,\n",
    "            feature_hint=FeatureHint.ITEM_ID,\n",
    "        ),\n",
    "        FeatureInfo(\n",
    "            column=\"rating\",\n",
    "            feature_type=FeatureType.NUMERICAL,\n",
    "            feature_hint=FeatureHint.RATING,\n",
    "        ),\n",
    "        FeatureInfo(\n",
    "            column=\"timestamp\",\n",
    "            feature_type=FeatureType.NUMERICAL,\n",
    "            feature_hint=FeatureHint.TIMESTAMP,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "polars_ratings = pl.from_pandas(pandas_ratings)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "spark_ratings = convert2spark(pandas_ratings)\n",
    "all_datasets = {\n",
    "        \"pandas\": Dataset(feature_schema, pandas_ratings),\n",
    "        \"polars\": Dataset(feature_schema, polars_ratings),\n",
    "        \"spark\": Dataset(feature_schema, spark_ratings)\n",
    "    }\n",
    "\n",
    "\n",
    "train_datasets = {\n",
    "        \"pandas\": Dataset(feature_schema, pandas_ratings[pandas_ratings[\"user_id\"]%10!=0]),\n",
    "        \"polars\": Dataset(feature_schema, polars_ratings.filter(pl.col(\"user_id\")%10!=0)),\n",
    "        \"spark\": Dataset(feature_schema, spark_ratings.filter(sf.col(\"user_id\")%10!=0))\n",
    "    }\n",
    "\n",
    "predict_datasets = {\n",
    "        \"pandas\": Dataset(feature_schema, pandas_ratings[pandas_ratings[\"user_id\"]%10==0]),\n",
    "        \"polars\": Dataset(feature_schema, polars_ratings.filter(pl.col(\"user_id\")%10==0)),\n",
    "        \"spark\": Dataset(feature_schema, spark_ratings.filter(sf.col(\"user_id\")%10==0))\n",
    "    }\n",
    "encoder = DatasetLabelEncoder()\n",
    "encoder.fit(all_datasets[\"spark\"])\n",
    "\n",
    "for framework in [\"spark\", \"pandas\", \"polars\"]:\n",
    "    train_datasets[framework] = encoder.transform(train_datasets[framework])\n",
    "    predict_datasets[framework] = encoder.transform(predict_datasets[framework])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_add_res(model, framework=\"spark\", suffix='', count_runs=100):\n",
    "    \"\"\"\n",
    "    Run fit_predict for the `model`, measure time on fit_predict and evaluate metrics\n",
    "    \"\"\"\n",
    "    fit_times = []\n",
    "    predict_times = []\n",
    "    for i in range(count_runs):\n",
    "        train_dataset, predict_dataset = train_datasets[framework], predict_datasets[framework]\n",
    "        start_time=time.time()\n",
    "        \n",
    "        logs = {'dataset': predict_dataset}\n",
    "        predict_params = {'k': K}\n",
    "        \n",
    "        predict_params.update(logs)\n",
    "\n",
    "        model.fit(train_dataset)\n",
    "        fit_time = time.time() - start_time\n",
    "\n",
    "        pred=model.predict(**predict_params)\n",
    "        if hasattr(model, \"_impl\") and model._get_implementation_type()==\"spark\":\n",
    "            pred.count()\n",
    "        predict_time = time.time() - start_time - fit_time\n",
    "        #print(f\"{fit_time=}\")\n",
    "        #print(f\"{predict_time=}\")\n",
    "        fit_times.append(fit_time)\n",
    "        predict_times.append(predict_time)\n",
    "    return fit_times, predict_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def full_pipeline(models, framework=\"spark\", suffix='', budget=BUDGET):\n",
    "    \"\"\"\n",
    "    For each model:\n",
    "        -  if required: run hyperparameters search, set best params and save param values to `experiment`\n",
    "        - pass model to `fit_predict_add_res`        \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for name, [model, params] in models.items():\n",
    "        fit_times, predict_times = fit_predict_add_res(model, framework, suffix)\n",
    "        fit_mean, fit_std, predict_mean, predict_std = (\n",
    "            np.array(fit_times).mean(), \n",
    "            np.array(fit_times).std(), \n",
    "            np.array(predict_times).mean(), \n",
    "            np.array(predict_times).std()\n",
    "        )\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"\\n{fit_mean=}\\n{fit_std=}\\n{predict_mean=}\\n{predict_std=}\")\n",
    "        results.append([name, (fit_mean, fit_std, predict_mean, predict_std)])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementation-only:\n",
      "\n",
      "fit_mean=0.024392220973968506\n",
      "fit_std=0.0018462059990847015\n",
      "predict_mean=0.14525318622589112\n",
      "predict_std=0.0075125902241277055\n",
      "Client+Implementation:\n",
      "\n",
      "fit_mean=0.024563350677490235\n",
      "fit_std=0.0018489209806340904\n",
      "predict_mean=0.14441093921661377\n",
      "predict_std=0.007492736718608101\n"
     ]
    }
   ],
   "source": [
    "models_to_compare_spark = {\n",
    "    \"Implementation-only\": [_PopRecPolars(), None],\n",
    "    \"Client+Implementation\": [PopRec(), None],\n",
    "}\n",
    "# !! think about how to restart SparkSession to calculate time on spark\n",
    "results = full_pipeline(models_to_compare_spark, framework=\"polars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 2 ways to use client:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### default - fit client and th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PopRec().fit_predict(all_datasets[\"spark\"], 1)\n",
    "PopRec().fit_predict(all_datasets[\"pandas\"], 1)\n",
    "PopRec().fit_predict(all_datasets[\"polars\"], 1)\n",
    "print(f\"{model.study}\")\n",
    "model.items_count\n",
    "model.item_popularity\n",
    "\n",
    "model = PopRec().fit(train_datasets[\"pandas\"])\n",
    "model.to_spark()\n",
    "res = model.predict(predict_datasets[\"spark\"], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PopRec()\n",
    "model._impl = _PopRecPandas()\n",
    "model.fit_items = pd.DataFrame([1, 2, 3])\n",
    "model.fit_queries = pd.DataFrame([1, 2])\n",
    "assert model._impl.fit_queries == model.fit_queries # True\n",
    "\n",
    "model._impl.can_predict_cold_items = False\n",
    "assert model.can_predict_cold_items == model._impl.can_predict_cold_items # True\n",
    "\n",
    "# If you need to change some attribute, and no setter - try code like this:\n",
    "model.can_predict_cold_items = True # RAISE Error - no setter\n",
    "model._impl.can_predict_cold_items = True\n",
    "assert model.can_predict_cold_items == True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "name": "movielens_nmf.ipynb",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "null"
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "2e7cb45cb9e942450ddf4e57ba99c929215192dab4b3e5d4b59b5877c1991cae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
